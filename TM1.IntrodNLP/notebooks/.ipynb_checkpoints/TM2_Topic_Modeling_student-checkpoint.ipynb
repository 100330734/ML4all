{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis and Topic Modelling\n",
    "\n",
    "Author: Jesús Cid Sueiro\n",
    "\n",
    "Date: 2016/04/03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore some tools for text analysis in python. To do so, first we will import the requested python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Required imports\n",
    "from wikitools import wiki\n",
    "from wikitools import category\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "from test_helper import Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Analysis\n",
    "\n",
    "The dictionary `D` and the Bag of Words in `corpus_bow` are the key inputs to the topic model algorithms. The topic model algorithms in `gensim` assume that input documents are parameterized using the tf-idf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, tfidf can be used to convert any vector from the old representation (bow integer counts) to the new one (TfIdf real-valued weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "tfidf[doc_bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to apply a transformation to a whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus_bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to apply a topic modeling algorithm. Latent Semantic Indexing is provided by `LsiModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Generate a LSI model with 5 topics for `corpus_tfidf` and dictionary `D`. You can check de sintaxis for [gensim.models.LsiModel](https://radimrehurek.com/gensim/models/lsimodel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize an LSI transformation\n",
    "n_topics = 5\n",
    "# scode: lsi = <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From LSI, we can check both the topic-tokens matrix and the document-topics matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the topics generated by LSI. An intuitive visualization is provided by the `show_topics` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.show_topics(num_topics=-1, num_words=10, log=False, formatted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a more useful representation of topics is as a list of tuples `(token, value)`. This is provided by the `show_topic` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Represent the columns of the topic-token matrix as a series of bar diagrams (one per topic) with the top 25 tokens of each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SORTED TOKEN FREQUENCIES (II):\n",
    "plt.rcdefaults()\n",
    "\n",
    "n_bins = 25\n",
    "\n",
    "# Example data\n",
    "y_pos = range(n_bins-1, -1, -1)\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 16, 8  # Set figure size\n",
    "\n",
    "for i in range(n_topics):\n",
    "\n",
    "    ### Plot top 25 tokens for topic i\n",
    "    # Read i-thtopic\n",
    "    # scode: <FILL IN>\n",
    "\n",
    "    # Plot\n",
    "    # scode: <FILL IN>\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI approximates any document as a linear combination of the topic vectors. We can compute the topic weights for any input corpus entered as input to the `lsi` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On real corpora, target dimensionality of\n",
    "# 200–500 is recommended as a “golden standard”\n",
    "# Create a double wrapper over the original \n",
    "# corpus bow  tfidf  fold-in-lsi\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "print corpus_lsi[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Find the document with the largest positive weight for topic 0. Compare the document and the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract weights from corpus_lsi\n",
    "# scode: weight0 = <FILL IN>\n",
    "\n",
    "# Locate the maximum positive weight\n",
    "nmax = np.argmax(weight0)\n",
    "print nmax\n",
    "print weight0[nmax]\n",
    "print corpus_lsi[nmax]\n",
    "\n",
    "# Get topic 0\n",
    "# scode: topic_0 = <FILL IN>\n",
    "\n",
    "# Compute a list of tuples (token, wordcount) for all tokens in topic_0, where wordcount is the number of \n",
    "# occurences of the token in the article.\n",
    "# scode: token_counts = <FILL IN>\n",
    "\n",
    "print \"Topic 0 is:\"\n",
    "print topic_0\n",
    "print \"Token counts:\"\n",
    "print token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several implementations of the LDA topic model in python:\n",
    "\n",
    "* Python library `lda`.\n",
    "* Gensim module: `gensim.models.ldamodel.LdaModel`\n",
    "* Sci-kit Learn module: `sklearn.decomposition`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. LDA using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the LDA module in `gensim` is similar to LSI. Furthermore, it assumes that a `tf-idf` parametrization is used as an input, which is not in complete agreement with the theoretical model, which assumes documents represented as vectors of token-counts.\n",
    "\n",
    "To use LDA in gensim, we must first create a lda model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldag = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus_tfidf, id2word=D, num_topics=10, update_every=1, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldag.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. LDA using Sci-kit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input matrix to the `sklearn` implementation of LDA contains the token-counts for all documents in the corpus.\n",
    "`sklearn` contains a powerfull `CountVectorizer` method that can be used to construct the input matrix from the `corpus_bow`. \n",
    "\n",
    "First, we will define an auxiliary function to print the top tokens in the model, that has been taken from the `sklearn` documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from an example in sklearn site \n",
    "# http://scikit-learn.org/dev/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "\n",
    "# You can try also with the dataset provided by sklearn in \n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                              remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a dataset to feed the Count_Vectorizer object, by joining all tokens in `corpus_clean` in a single string, using a space ' ' as separator.\n",
    "\n",
    "**Task**: Join all tokens from each document in a single string, using a white space as separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "print(\"Loading dataset...\")\n",
    "# scode: data_samples = <FILL IN>\n",
    "print 'Document 0:'\n",
    "print data_samples[0][0:200], '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to compute the token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print tf[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the LDA algorithm. \n",
    "\n",
    "**Task**: Create an LDA object with the following parameters: \n",
    "    n_topics=n_topics, max_iter=5,\n",
    "    learning_method='online',\n",
    "    learning_offset=50.,\n",
    "    random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "# scode: lda = <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Fit model `lda` with the token frequencies computed by `tf_vectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Represent graphically the topic distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Explore the influence of the concentration parameters, $alpha$ (`doc_topic_prior` in `sklearn`) and $eta$(`topic_word_prior`). In particular observe how do topic and document distributions change as these parameters increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise**: The token dictionary and the token distribution have shown that:\n",
    "\n",
    "1. Some tokens, despite being very frequent in the corpus, have no semantic relevance for topic modeling. Unfortunately, they were not present in the stopword list, and have not been elliminated before the analysis.\n",
    "\n",
    "2. A large portion of tokens appear only once and, thus, they are not statistically relevant for the inference engine of the topic models.\n",
    "\n",
    "Revise the entire corpus be removing from the corpus all these sets of terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise**: Note that we have not used the terms in the article titles, though the can be expected to containg relevant words for the topic modeling. Include the title words in the analyisis. In order to give them a special relevante, insert them in the corpus several time, so as to make their words more significant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise**: The topic modelling algorithms we have tested in this notebook are non-supervised. This makes them difficult to evaluate objectivelly. In order to test if LDA captures real topics, construct a dataset as the mixture of wikipedia articles from 4 different categories, and test if LDA with 4 topics identifies topics closely related to the original categories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
