{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entity Resolution**\n",
    "\n",
    "#### Entity Resolution is the term to describe the process of linking records from one data source with another that describe the same entity. \n",
    "\n",
    "#### We will use data files from the [metric-learning](https://code.google.com/p/metric-learning/) project:\n",
    "\n",
    "* **Google.txt**, the Google Products dataset\n",
    "* **Amazon.txt**, the Amazon dataset\n",
    "* **Google_small.txt**, 200 records sampled from the Google data\n",
    "* **Amazon_small.txt**, 200 records sampled from the Amazon data\n",
    "* **stopwords.txt**, a list of common English words\n",
    "\n",
    "#### **EXERCISE:** Load the datasets directly into RDDs using sc.textFile. Use 4 partitions of the data.\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "<pre><code>\n",
    "Loaded googleSmall dataset with size 200\n",
    "Loaded amazonSmall dataset with size 200\n",
    "Loaded google dataset with size 3226\n",
    "Loaded amazon dataset with size 1363\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/vagrant/synced_folder/Notebooks/Topic Modeling/notebook/data/googleSmall.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:58)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-02a55a6e6ba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgoogleSmallRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/googleSmall.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Loaded googleSmall dataset with size %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgoogleSmallRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mamazonSmallRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/amazonSmall.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Loaded amazonSmall dataset with size %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mamazonSmallRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \"\"\"\n\u001b[1;32m-> 1004\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m         \"\"\"\n\u001b[1;32m--> 995\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \"\"\"\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/vagrant/synced_folder/Notebooks/Topic Modeling/notebook/data/googleSmall.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:58)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "googleSmallRDD = sc.textFile('data/googleSmall.txt', 4)          \n",
    "print \"Loaded googleSmall dataset with size %d\" % googleSmallRDD.count()\n",
    "\n",
    "amazonSmallRDD = sc.textFile('data/amazonSmall.txt', 4)          \n",
    "print \"Loaded amazonSmall dataset with size %d\" % amazonSmallRDD.count()\n",
    "\n",
    "googleRDD = sc.textFile('data/google.txt', 4)          \n",
    "print \"Loaded google dataset with size %d\" % googleRDD.count()\n",
    "\n",
    "amazonRDD = sc.textFile('data/amazon.txt', 4)          \n",
    "print \"Loaded amazon dataset with size %d\" % amazonRDD.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's examine some of the lines in the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the aspect of lines from the google dataset:\n",
      " \n",
      "ID: http://www.google.com/base/feeds/snippets/11448761432933644608\n",
      "CONTENT: spanish vocabulary builder \"expand your vocabulary! contains fun lessons that both teach and entertain you'll quickly find yourself mastering new terms. includes games and more!\" \n",
      " \n",
      "ID: http://www.google.com/base/feeds/snippets/8175198959985911471\n",
      "CONTENT: topics presents: museums of world \"5 cd-rom set. step behind the velvet rope to examine some of the most treasured collections of antiquities art and inventions. includes the following the louvre - virtual visit 25 rooms in full screen interactive video detailed map of the louvre ...\" \n",
      " \n",
      "ID: http://www.google.com/base/feeds/snippets/18445827127704822533\n",
      "CONTENT: sierrahome hse hallmark card studio special edition win 98 me 2000 xp \"hallmark card studio special edition (win 98 me 2000 xp)\" \"sierrahome\"\n",
      " \n",
      "This is the aspect of lines from the amazon dataset:\n",
      " \n",
      "ID: b000jz4hqo\n",
      "CONTENT: clickart 950 000 - premier image pack (dvd-rom)  \"broderbund\"\n",
      " \n",
      "ID: b0006zf55o\n",
      "CONTENT: ca international - arcserve lap/desktop oem 30pk \"oem arcserve backup v11.1 win 30u for laptops and desktops\" \"computer associates\"\n",
      " \n",
      "ID: b00004tkvy\n",
      "CONTENT: noah's ark activity center (jewel case ages 3-8)  \"victory multimedia\"\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print \"This is the aspect of lines from the google dataset:\"\n",
    "print \" \"\n",
    "\n",
    "for line in googleSmallRDD.take(3):\n",
    "    fields = line.split(';')\n",
    "    print \"ID: %s\" % fields[0]\n",
    "    print \"CONTENT: %s\" % fields[1]\n",
    "    print \" \"\n",
    "\n",
    "print \"This is the aspect of lines from the amazon dataset:\"\n",
    "print \" \"\n",
    "\n",
    "for line in amazonSmallRDD.take(3):\n",
    "    fields = line.split(';')\n",
    "    print \"ID: %s\" % fields[0]\n",
    "    print \"CONTENT: %s\" % fields[1]\n",
    "    print \" \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In what follows we will use the ID as a key, and the content will be the text needed for matching among products. The similarity measure will be a cosine distance operating on [Bag-of-words][bag-of-words]. In the next sections we will learn to implement such a similarity.\n",
    "\n",
    "#### First of all, we need to build a function to transform a string into a list of terms (tokens). \n",
    "\n",
    "#### **EXERCISE:** Complete the definition of the 'tokeniza' function, and do not forget: \n",
    "* to lower case the text\n",
    "* to remove punctuation signs\n",
    "* to eliminate empty tokens\n",
    "\n",
    "#### You may want to use regular expressions, at  [regex101](https://regex101.com/)  you can explore regular expressions on strings.\n",
    "\n",
    "[bag-of-words]: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "<pre><code>\n",
    "This is the tokenized text:\n",
    "\n",
    "['the', 'bag', 'of', 'words', 'model', 'is', 'a', 'simplifying', 'representation', 'used', 'in', 'natural', 'language', 'processing', 'and', 'information', 'etrieval', 'ir', 'in', 'this', 'model', 'a', 'text', 'such', 'as', 'a', 'sentence', 'or', 'a', 'document', 'is', 'represented', 'as', 'the', 'bag', 'multiset', 'of', 'its', 'words', 'disregarding', 'grammar', 'and', 'even', 'word', 'order', 'but', 'keeping', 'multiplicity']\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the tokenized text:\n",
      "\n",
      "['the', 'bag', 'of', 'words', 'model', 'is', 'a', 'simplifying', 'representation', 'used', 'in', 'natural', 'language', 'processing', 'and', 'information', 'etrieval', 'ir', 'in', 'this', 'model', 'a', 'text', 'such', 'as', 'a', 'sentence', 'or', 'a', 'document', 'is', 'represented', 'as', 'the', 'bag', 'multiset', 'of', 'its', 'words', 'disregarding', 'grammar', 'and', 'even', 'word', 'order', 'but', 'keeping', 'multiplicity']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "texto = \"The bag-of-words model is a simplifying representation used in natural language processing and information \\\\\n",
    "retrieval (IR).  In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of\\\\\n",
    "its words, disregarding grammar and even word order but keeping multiplicity\"\n",
    "\n",
    "def tokeniza(string):\n",
    "    RE = r'\\W+' # match any non-word character [^a-zA-Z0-9_], and removes punctuation signs.\n",
    "    tokens = re.split(RE, string.lower())\n",
    "    tokens = [t for t in tokens if len(t) > 0]\n",
    "    return tokens\n",
    "\n",
    "print \"This is the tokenized text:\\n\"\n",
    "print tokeniza(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is important to eliminate the [Stopwords][stopwords], that are common words that do not contribute much to the meaning of a document (e.g., \"the\", \"a\", \"is\", \"to\", etc.). \n",
    "\n",
    "#### **EXERCISE:** Load the file \"stopwords.txt\" and use them to improve the tokeniza function.\n",
    "[stopwords]: https://en.wikipedia.org/wiki/Stop_words\n",
    "\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "<pre><code>\n",
    "This is the tokenized text without stopwords:\n",
    "\n",
    "['bag', 'words', 'model', 'simplifying', 'representation', 'used', 'natural', 'language', 'processing', 'information', 'etrieval', 'ir', 'model', 'text', 'sentence', 'document', 'represented', 'bag', 'multiset', 'words', 'disregarding', 'grammar', 'even', 'word', 'order', 'keeping', 'multiplicity']\n",
    "</code></pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the english stopwords:\n",
      "\n",
      "all just being over both through yourselves its before with had should to only under ours has do them his very they not during now him nor did these t each where because doing theirs some are our ourselves out what for below does above between she be we after here hers by on about of against s or own into yourself down your from her whom there been few too themselves was until more himself that but off herself than those he me myself this up will while can were my and then is in am it an as itself at have further their if again no when same any how other which you who most such why a don i having so the yours once  \n",
      "\n",
      "\n",
      "This is the tokenized text without stopwords:\n",
      "\n",
      "['bag', 'words', 'model', 'simplifying', 'representation', 'used', 'natural', 'language', 'processing', 'information', 'etrieval', 'ir', 'model', 'text', 'sentence', 'document', 'represented', 'bag', 'multiset', 'words', 'disregarding', 'grammar', 'even', 'word', 'order', 'keeping', 'multiplicity']\n"
     ]
    }
   ],
   "source": [
    "stopfile = 'data/stopwords.txt'\n",
    "stopwords = list(set(sc.textFile(stopfile).collect()))\n",
    "print 'These are the english stopwords:\\n'\n",
    "for sw in stopwords:\n",
    "    print sw, \n",
    "print \" \\n\\n\"\n",
    "\n",
    "def tokeniza(string, stopwords):\n",
    "    RE = r'\\W+' # match any non-word character [^a-zA-Z0-9_]\n",
    "    tokens = re.split(RE, string.lower())\n",
    "    tokens = [t for t in tokens if len(t) > 0 and t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "print \"This is the tokenized text without stopwords:\\n\"\n",
    "print tokeniza(texto, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the RDDS\n",
    "\n",
    "#### We want to modify the structure of the RDDs such that every item has now the format (ID, [list of tokens])\n",
    "\n",
    "#### **EXERCISE:**  Use the 'tokeniza' function to tokenize the amazonSmall RDD and count the number of unique tokens in that RDD. Repeat for the other datasets. It is better you define a 'tokenizeRDD' function that takes a RDD as argument (plus the stopwords) and returns a modified RDD.\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "<pre><code>\n",
    "These are the first 5 elements of the processed amazonSmallRDD:\n",
    "\n",
    "('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund'])\n",
    " \n",
    "('b0006zf55o', ['ca', 'international', 'arcserve', 'lap', 'desktop', 'oem', '30pk', 'oem', 'arcserve', 'backup', 'v11', '1', 'win', '30u', 'laptops', 'desktops', 'computer', 'associates'])\n",
    " \n",
    "('b00004tkvy', ['noah', 'ark', 'activity', 'center', 'jewel', 'case', 'ages', '3', '8', 'victory', 'multimedia'])\n",
    " \n",
    "('b000g80lqo', ['peachtree', 'sage', 'premium', 'accounting', 'nonprofits', '2007', 'peachtree', 'premium', 'accounting', 'nonprofits', '2007', 'affordable', 'easy', 'use', 'accounting', 'solution', 'provides', 'donor', 'grantor', 'management', 're', 'like', 'nonprofit', 'organizations', 're', 'constantly', 'striving', 'maximize', 'every', 'dollar', 'annual', 'operating', 'budget', 'financial', 'reporting', 'programs', 'funds', 'advanced', 'operational', 'reporting', 'rock', 'solid', 'core', 'accounting', 'features', 'made', 'peachtree', 'choice', 'hundreds', 'thousands', 'small', 'businesses', 'result', 'accounting', 'solution', 'tailor', 'made', 'challenges', 'operating', 'nonprofit', 'organization', 'keep', 'audit', 'trail', 'record', 'report', 'changes', 'made', 'transactions', 'improve', 'data', 'integrity', 'prior', 'period', 'locking', 'archive', 'organization', 'data', 'snap', 'shots', 'data', 'closed', 'year', 'set', 'individual', 'user', 'profiles', 'password', 'protection', 'peachtree', 'restore', 'wizard', 'restores', 'backed', 'data', 'files', 'plus', 'web', 'transactions', 'customized', 'forms', 'includes', 'standard', 'accounting', 'features', 'general', 'ledger', 'accounts', 'receivable', 'accounts', 'payable'])\n",
    " \n",
    "('b0006se5bq', ['singing', 'coach', 'unlimited', 'singing', 'coach', 'unlimited', 'electronic', 'learning', 'products', 'win', 'nt', '2000', 'xp', 'carry', 'tune', 'technologies'])\n",
    "</code></pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with amazonSmallRDD.\n",
      "Finished with amazonRDD.\n",
      "Finished with googleSmallRDD.\n",
      "Finished with googleRDD.\n",
      "\n",
      "These are the first 5 elements of the processed amazonSmallRDD:\n",
      "\n",
      "(u'b000jz4hqo', [u'clickart', u'950', u'000', u'premier', u'image', u'pack', u'dvd', u'rom', u'broderbund'])\n",
      " \n",
      "(u'b0006zf55o', [u'ca', u'international', u'arcserve', u'lap', u'desktop', u'oem', u'30pk', u'oem', u'arcserve', u'backup', u'v11', u'1', u'win', u'30u', u'laptops', u'desktops', u'computer', u'associates'])\n",
      " \n",
      "(u'b00004tkvy', [u'noah', u'ark', u'activity', u'center', u'jewel', u'case', u'ages', u'3', u'8', u'victory', u'multimedia'])\n",
      " \n",
      "(u'b000g80lqo', [u'peachtree', u'sage', u'premium', u'accounting', u'nonprofits', u'2007', u'peachtree', u'premium', u'accounting', u'nonprofits', u'2007', u'affordable', u'easy', u'use', u'accounting', u'solution', u'provides', u'donor', u'grantor', u'management', u're', u'like', u'nonprofit', u'organizations', u're', u'constantly', u'striving', u'maximize', u'every', u'dollar', u'annual', u'operating', u'budget', u'financial', u'reporting', u'programs', u'funds', u'advanced', u'operational', u'reporting', u'rock', u'solid', u'core', u'accounting', u'features', u'made', u'peachtree', u'choice', u'hundreds', u'thousands', u'small', u'businesses', u'result', u'accounting', u'solution', u'tailor', u'made', u'challenges', u'operating', u'nonprofit', u'organization', u'keep', u'audit', u'trail', u'record', u'report', u'changes', u'made', u'transactions', u'improve', u'data', u'integrity', u'prior', u'period', u'locking', u'archive', u'organization', u'data', u'snap', u'shots', u'data', u'closed', u'year', u'set', u'individual', u'user', u'profiles', u'password', u'protection', u'peachtree', u'restore', u'wizard', u'restores', u'backed', u'data', u'files', u'plus', u'web', u'transactions', u'customized', u'forms', u'includes', u'standard', u'accounting', u'features', u'general', u'ledger', u'accounts', u'receivable', u'accounts', u'payable'])\n",
      " \n",
      "(u'b0006se5bq', [u'singing', u'coach', u'unlimited', u'singing', u'coach', u'unlimited', u'electronic', u'learning', u'products', u'win', u'nt', u'2000', u'xp', u'carry', u'tune', u'technologies'])\n",
      " \n"
     ]
    }
   ],
   "source": [
    "amazonSmallRDD.take(1)\n",
    "\n",
    "def tokenizeRDD(RDD, stopwords):\n",
    "    tokenizedRDD = (RDD\n",
    "                         .map(lambda x: x.split(';'))\n",
    "                         .map(lambda x: (x[0], tokeniza(x[1], stopwords)))\n",
    "                         .cache()\n",
    "                     )\n",
    "    return tokenizedRDD\n",
    "\n",
    "amazonSmallRDDtokenized = tokenizeRDD(amazonSmallRDD, stopwords)\n",
    "print \"Finished with amazonSmallRDD.\"\n",
    "amazonRDDtokenized = tokenizeRDD(amazonRDD, stopwords)\n",
    "print \"Finished with amazonRDD.\"\n",
    "googleSmallRDDtokenized = tokenizeRDD(googleSmallRDD, stopwords)\n",
    "print \"Finished with googleSmallRDD.\"\n",
    "googleRDDtokenized = tokenizeRDD(googleRDD, stopwords)\n",
    "print \"Finished with googleRDD.\\n\"\n",
    "\n",
    "print \"These are the first 5 elements of the processed amazonSmallRDD:\\n\"\n",
    "elements = amazonSmallRDDtokenized.take(5)\n",
    "for l in elements:\n",
    "    print l\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting tokens: we want to know the vocabulary size in every one of the datasets. \n",
    "\n",
    "#### **EXERCISE:**  Count the number of tokens in every dataset. Also count the number of UNIQUE tokens in every dataset.\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "\n",
    "<pre><code>\n",
    "amazonSmall has 14052 tokens.\n",
    "amazon has 133267 tokens.\n",
    "googleSmall has 5710 tokens.\n",
    "google has 98588 tokens.\n",
    " \n",
    "amazonSmall has 3700 unique tokens.\n",
    "amazon has 11505 unique tokens.\n",
    "googleSmall has 2305 unique tokens.\n",
    "google has 11176 unique tokens.\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazonSmall has 14052 tokens.\n",
      "amazon has 133267 tokens.\n",
      "googleSmall has 5710 tokens.\n",
      "google has 98588 tokens.\n",
      " \n",
      "amazonSmall has 3700 unique tokens.\n",
      "amazon has 11505 unique tokens.\n",
      "googleSmall has 2305 unique tokens.\n",
      "google has 11176 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "NtokensamazonSmall = amazonSmallRDDtokenized.map(lambda x: len(x[1])).sum()\n",
    "print \"amazonSmall has %d tokens.\" % NtokensamazonSmall\n",
    "\n",
    "Ntokensamazon = amazonRDDtokenized.map(lambda x: len(x[1])).sum()\n",
    "print \"amazon has %d tokens.\" % Ntokensamazon\n",
    "\n",
    "NtokensgoogleSmall = googleSmallRDDtokenized.map(lambda x: len(x[1])).sum()\n",
    "print \"googleSmall has %d tokens.\" % NtokensgoogleSmall\n",
    "\n",
    "Ntokensgoogle = googleRDDtokenized.map(lambda x: len(x[1])).sum()\n",
    "print \"google has %d tokens.\" % Ntokensgoogle\n",
    "\n",
    "print \" \"\n",
    "\n",
    "NtokensuniqueamazonSmall = len(amazonSmallRDDtokenized.map(lambda x: x[1]).reduce(lambda x, y: list(set(x + y))))\n",
    "print \"amazonSmall has %d unique tokens.\" % NtokensuniqueamazonSmall\n",
    "\n",
    "Ntokensuniqueamazon = len(amazonRDDtokenized.map(lambda x: x[1]).reduce(lambda x, y: list(set(x + y))))\n",
    "print \"amazon has %d unique tokens.\" % Ntokensuniqueamazon\n",
    "\n",
    "NtokensuniquegoogleSmall = len(googleSmallRDDtokenized.map(lambda x: x[1]).reduce(lambda x, y: list(set(x + y))))\n",
    "print \"googleSmall has %d unique tokens.\" % NtokensuniquegoogleSmall\n",
    "\n",
    "Ntokensuniquegoogle = len(googleRDDtokenized.map(lambda x: x[1]).reduce(lambda x, y: list(set(x + y))))\n",
    "print \"google has %d unique tokens.\" % Ntokensuniquegoogle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Largest and smallest item in amazon dataset. \n",
    "\n",
    "#### **EXERCISE:**  Build an 'amazonCountRDD' that stores (ID, [Tokens], Number of tokens). Print the largest content (the record with the largest number of tokens) and the smallest content (the one with the smaller number of tokens). \n",
    "\n",
    "#### **_The answer should be_:**\n",
    "\n",
    "<pre><code>\n",
    "amazon smallest item, with length 2, is:\n",
    "[('b000hlt5g2', ['monopoly', 'encore'], 2)]\n",
    " \n",
    "amazon largest item, with length 1520, is:\n",
    "[('b0007lw22q', ['apple', 'ilife', '06', 'family', 'pack', 'mac', 'dvd', 'older', ....\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon smallest item, with length 2, is:\n",
      "[(u'b000hlt5g2', [u'monopoly', u'encore'], 2)]\n",
      " \n",
      "amazon largest item, with length 1520, is:\n",
      "[(u'b0007lw22q', [u'apple', u'ilife', u'06', u'family', u'pack', u'mac', u'dvd', u'older', u'version', u'ilife', u'06', u'easiest', u'way', u'make', u'every', u'bit', u'digital', u'life', u'use', u'mac', u'collect', u'organize', u'edit', u'various', u'elements', u'transform', u'mouth', u'watering', u'masterpieces', u'apple', u'designed', u'templates', u'share', u'magic', u'moments', u'beautiful', u'books', u'colorful', u'calendars', u'dazzling', u'dvds', u'perfect', u'podcasts', u'attractive', u'online', u'journals', u'starring', u'family', u'pack', u'lets', u'install', u'ilife', u'06', u'five', u'apple', u'computers', u'household', u'easier', u'ever', u'edit', u'photos', u'perfection', u'photos', u'one', u'place', u'iphoto', u'6', u'rebuilt', u'blazing', u'performance', u'iphoto', u'makes', u'sharing', u'photos', u'faster', u'simpler', u'cooler', u'ever', u'also', u'adds', u'eye', u'opening', u'features', u'ones', u'already', u'love', u'including', u'photocasting', u'support', u'250', u'000', u'photos', u'easy', u'publishing', u'web', u'special', u'effects', u'new', u'custom', u'cards', u'calendars', u'spread', u'smiles', u'far', u'wide', u'lifetime', u'photos', u'fingertips', u'life', u'one', u'big', u'photo', u'opportunity', u'explains', u'photo', u'library', u'getting', u'bigger', u'every', u'day', u'well', u'good', u'news', u'iphoto', u'supports', u'250', u'000', u'images', u'means', u'confidently', u'shoot', u'thousand', u'photos', u'per', u'week', u'next', u'20', u'years', u'better', u'get', u'cracking', u'navigating', u'larger', u'library', u'breeze', u'move', u'scroll', u'bar', u'new', u'see', u'scroll', u'guide', u'appears', u'show', u'photos', u'rolls', u'currently', u'displayed', u'photocasting', u'fantastic', u'new', u'way', u'share', u'imagine', u'sending', u'album', u'favorite', u'photos', u'family', u'change', u'automatically', u'computers', u'update', u'photocasting', u'amazingly', u'easy', u'create', u'photocast', u'album', u'iphoto', u'publish', u'mac', u'password', u'protected', u'wish', u'cousin', u'cindy', u'subscribes', u'll', u'see', u'iphoto', u'library', u'see', u'full', u'resolution', u'photos', u'used', u'desktop', u'pictures', u'printed', u'albums', u'cards', u'calendars', u'whenever', u'add', u'subtract', u'photocast', u'album', u'change', u'even', u'people', u'pcs', u'enjoy', u'iphoto', u'photocast', u'takes', u'rss', u'news', u'reader', u'subscribe', u'dazzle', u'calendars', u'greeting', u'cards', u'books', u'send', u'beautiful', u'printed', u'book', u'special', u'photos', u'friend', u'put', u'kid', u'happy', u'face', u'cover', u'custom', u'card', u'announce', u'birthday', u'party', u'create', u'stylized', u'personalized', u'calendar', u'rivals', u'ones', u'see', u'local', u'mall', u'make', u'things', u'many', u'many', u'typical', u'iphoto', u'ease', u'choose', u'photo', u'album', u'click', u'apple', u'designed', u'template', u'let', u'iphoto', u'work', u'magic', u'add', u'captions', u'fine', u'tune', u'layout', u'click', u'order', u'professionally', u'printed', u'masterwork', u'delivered', u'right', u'door', u'website', u'life', u'one', u'thing', u'share', u'photos', u'internet', u'something', u'else', u'share', u'wonderfully', u'designed', u'personal', u'website', u'thanks', u'newest', u'member', u'ilife', u'family', u'iweb', u'create', u'beautiful', u'new', u'photo', u'page', u'website', u'minutes', u'heck', u'type', u'clever', u'captions', u'quickly', u'enough', u'maybe', u'even', u'seconds', u'simply', u'export', u'iphoto', u'album', u'iweb', u'create', u'new', u'photo', u'grid', u'drag', u'photos', u'rearrange', u'needed', u'add', u'pithy', u'words', u'publish', u'whoosh', u'film', u'way', u'done', u'easily', u'imovie', u'hd', u'6', u'score', u'movies', u'powerful', u'audio', u'tools', u'imovie', u'hd', u'6', u'riveting', u'performances', u'major', u'effects', u'inspired', u'directing', u'amazing', u'imovie', u'changed', u'way', u'people', u'look', u'home', u'movies', u'cinematic', u'tools', u'offer', u'imovie', u'makes', u'even', u'comfortable', u'director', u'chair', u'editor', u'chair', u'special', u'effects', u'guy', u'chair', u'well', u'get', u'idea', u'imovie', u'fastest', u'easiest', u'way', u'turn', u'home', u'movies', u'dazzling', u'hollywood', u'style', u'hits', u'instant', u'theme', u'instant', u'classic', u'imovie', u'themes', u'give', u'moviemaking', u'power', u'never', u'imagined', u'click', u'one', u'fun', u'begins', u'theme', u'contains', u'collection', u'professionally', u'designed', u'scenes', u'give', u'movie', u'personality', u'start', u'finish', u'including', u'video', u'graphic', u'overlays', u'advanced', u'transitions', u'drag', u'drop', u'movie', u'clips', u'photos', u'scene', u'drop', u'zones', u'type', u'titles', u'imovie', u'rest', u'get', u'quality', u'feature', u'film', u'without', u'cost', u'overruns', u'showing', u'major', u'video', u'effects', u'll', u'love', u'new', u'video', u'effects', u'made', u'possible', u'mac', u'os', u'x', u'core', u'video', u'll', u'also', u'love', u'fact', u'preview', u'results', u'video', u'effect', u'choices', u'real', u'time', u'size', u'right', u'main', u'window', u'imovie', u'rendering', u'background', u'experiment', u'heart', u'content', u'dive', u'right', u'creative', u'options', u'without', u'delay', u'sounds', u'cinema', u'great', u'movies', u'sound', u'amazing', u'look', u'imovie', u'comes', u'complete', u'sound', u'studio', u'built', u'summoning', u'power', u'mac', u'os', u'x', u'core', u'audio', u'imovie', u'offers', u'eight', u'new', u'audio', u'effects', u'including', u'noise', u'reduction', u'perfect', u'squelching', u'noise', u'common', u'home', u'movies', u'reverb', u'pitch', u'change', u'handy', u'graphic', u'equalizer', u'ilife', u'media', u'browser', u'full', u'access', u'original', u'garageband', u'songs', u'sound', u'video', u'podcasts', u'blogs', u'iweb', u'use', u'imovie', u'share', u'friends', u'family', u'world', u'imovie', u'working', u'hand', u'hand', u'iweb', u'makes', u'easy', u'publish', u'video', u'websites', u'blogs', u'even', u'use', u'imovie', u'create', u'video', u'podcasts', u'complete', u'chapter', u'markers', u'live', u'urls', u'using', u'iweb', u'submit', u'video', u'podcast', u'itunes', u'podcast', u'directory', u'seen', u'subscribed', u'everyone', u'simple', u'powerful', u'idvd', u'interface', u'choose', u'stunning', u'menu', u'templates', u'idvd', u'6', u'rent', u'someone', u'else', u'masterpiece', u'create', u'hollywood', u'style', u'home', u'movies', u'multimedia', u'wedding', u'albums', u'professional', u'slideshow', u'portfolios', u'idvd', u'6', u'helps', u'put', u'dvd', u'ordinary', u'dvd', u'mind', u'jaw', u'dropping', u'widescreen', u'dvd', u'coordinated', u'menus', u'ambient', u'audio', u'dvd', u'thoroughly', u'professional', u'polish', u'dvd', u'captivating', u'make', u'onto', u'everyone', u'must', u'see', u'list', u'magic', u'idvd', u'idvd', u'always', u'made', u'easy', u'create', u'beautifully', u'designed', u'dvds', u'beyond', u'easy', u'magic', u'idvd', u'feature', u'choose', u'theme', u'select', u'movies', u'photos', u'want', u'include', u'idvd', u'automatically', u'creates', u'complete', u'dvd', u'unified', u'design', u'start', u'finish', u'including', u'menu', u'screens', u'movies', u'chapter', u'menus', u'slideshows', u'see', u'believe', u'want', u'join', u'magic', u'use', u'magic', u'idvd', u'starting', u'point', u'edit', u'create', u'widescreen', u'dvds', u'get', u'gorgeous', u'new', u'widescreen', u'tv', u'idvd', u'6', u'author', u'dvds', u'movies', u'photo', u'slideshows', u'stunning', u'widescreen', u'format', u'even', u'include', u'content', u'sd', u'hd', u'video', u'sources', u'idvd', u'converts', u'everything', u'automatically', u'dvds', u'play', u'back', u'beautifully', u'choose', u'new', u'idvd', u'themes', u'idvd', u'6', u'features', u'10', u'new', u'apple', u'designed', u'themes', u'idvd', u'five', u'made', u'match', u'imovie', u'themes', u'available', u'widescreen', u'16', u'9', u'standard', u'4', u'3', u'formats', u'theme', u'includes', u'family', u'three', u'coordinated', u'menus', u'main', u'menu', u'chapter', u'menu', u'scene', u'selection', u'extras', u'menu', u'slideshows', u'content', u'talent', u'burn', u're', u'ready', u'burn', u'dvd', u'idvd', u'ready', u'even', u're', u'using', u'third', u'party', u'dvd', u'burner', u'idvd', u'built', u'support', u'wide', u'variety', u'dvd', u'media', u'formats', u'including', u'dvd', u'r', u'dvd', u'rw', u'dvd', u'r', u'dvd', u'rw', u'dvd', u'r', u'dl', u'options', u'ever', u'garageband', u'lets', u'make', u'music', u'like', u'pro', u'mac', u'create', u'podcasts', u'make', u'sound', u'like', u'professional', u'host', u'score', u'imovie', u'creations', u'garageband', u'3', u'best', u'way', u'record', u'music', u'mac', u'best', u'way', u'record', u'podcasts', u'podcasting', u'garageband', u'3', u'puts', u'control', u'room', u'full', u'featured', u'radio', u'station', u'new', u'iweb', u'integration', u'gets', u'voice', u'internet', u'minutes', u'podcast', u'artwork', u'track', u'add', u'podcast', u'artwork', u'track', u'dragging', u'images', u'ilife', u'media', u'browser', u'drag', u'title', u'card', u'name', u'podcast', u'picture', u'drag', u'different', u'images', u'chapter', u'marker', u'podcast', u'listeners', u'also', u'see', u'visual', u'cues', u'position', u'images', u'artwork', u'track', u'correspond', u'vocal', u'track', u'raving', u'amazing', u'unsigned', u'band', u'saw', u'last', u'night', u'drag', u'photo', u'gig', u'right', u'iphoto', u'library', u'sound', u'effects', u'jingles', u'give', u'podcast', u'professional', u'polish', u'adding', u'sound', u'effects', u'jingles', u'garageband', u'library', u'200', u'podcast', u'sounds', u'browse', u'200', u'sound', u'effects', u'including', u'radio', u'style', u'stingers', u'sounds', u'people', u'animals', u'machines', u'drag', u'podcast', u'sync', u'vocal', u'track', u'add', u'musical', u'accompaniment', u'podcast', u'browse', u'garageband', u'library', u'100', u'jingles', u'drag', u'podcast', u'7', u'15', u'30', u'second', u'snippets', u'add', u'sound', u'effects', u'jingles', u'post', u'production', u'trigger', u'live', u'recording', u'podcast', u'radio', u'engineer', u'garageband', u'3', u'includes', u'features', u'function', u'like', u'personal', u'podcast', u'radio', u'engineer', u'create', u'podcasts', u'make', u'sound', u'like', u'professional', u'host', u'built', u'speech', u'enhancer', u'optimizes', u'sound', u'mac', u'gender', u'vocal', u'range', u'improving', u'sound', u'voice', u'simulating', u'professional', u'microphone', u'even', u're', u'using', u'one', u'dynamic', u'ducking', u'effect', u'automatically', u'reduces', u'music', u'volume', u'speak', u'listeners', u'always', u'hear', u'talk', u'tunes', u'ichat', u'interview', u'recording', u'use', u'ichat', u'garageband', u'record', u'talk', u'show', u'style', u'podcast', u'time', u'takes', u'carry', u'friendly', u'chat', u'even', u'guests', u'side', u'world', u'garageband', u'lets', u'record', u'remote', u'guests', u'audio', u'video', u'ichat', u'conference', u'start', u'chatting', u'garageband', u'simultaneously', u'records', u'audio', u'one', u'track', u'guest', u'complete', u'buddy', u'name', u'icon', u'know', u'saying', u're', u'using', u'isight', u'cameras', u'record', u'action', u'garageband', u'takes', u'photo', u'snapshot', u'guest', u'every', u'time', u'speaks', u'one', u'click', u'iweb', u'publishing', u'finished', u'recording', u'podcast', u'time', u'get', u'internet', u'garageband', u'iweb', u'garageband', u'send', u'podcast', u'iweb', u'create', u'new', u'podcast', u'series', u'add', u'existing', u'series', u'click', u'publish', u'get', u'podcast', u'web', u'via', u'mac', u'account', u'easy', u'iweb', u'even', u'lets', u'submit', u'podcast', u'itunes', u'music', u'store', u'attract', u'new', u'fans', u'imovie', u'scoring', u'new', u'video', u'track', u'garageband', u'makes', u'easy', u'add', u'original', u'music', u'score', u'movies', u'worry', u'musical', u'talent', u'lack', u'thereof', u'use', u'garageband', u'included', u'loops', u'try', u'combination', u'loops', u'software', u'instruments', u'previous', u'audio', u'recordings', u'created', u'even', u'use', u'garageband', u'add', u'cinematic', u'sound', u'effects', u'footsteps', u'creaking', u'doors', u're', u'ready', u'save', u'scored', u'imovie', u'project', u'quicktime', u'video', u'send', u'idvd', u'burning', u'publish', u'internet', u'via', u'iweb', u'internet', u'calling', u'answer', u'iweb', u'choose', u'beautiful', u'website', u'templates', u'publish', u'blog', u'quickly', u'easily', u'iweb', u'internet', u'calling', u'answer', u'use', u'iweb', u'create', u'websites', u'blogs', u'complete', u'podcasts', u'photos', u'movies', u'get', u'online', u'fast', u'drag', u'drop', u'design', u'using', u'choice', u'web', u'templates', u'publish', u'live', u'mac', u'account', u'apple', u'designed', u'templates', u'let', u'iweb', u'help', u'build', u'beautiful', u'website', u'minutes', u'using', u'apple', u'designed', u'templates', u'choose', u'website', u'theme', u'fits', u'style', u'theme', u'offers', u'page', u'templates', u'photo', u'album', u'blog', u'podcast', u'movie', u'pages', u'll', u'always', u'perfect', u'place', u'content', u'use', u'ilife', u'media', u'browser', u'drag', u'photos', u'movies', u'podcasts', u'simply', u'type', u'placeholder', u'text', u'page', u'template', u'click', u'publish', u'mac', u'ilife', u'media', u'browser', u'every', u'website', u'needs', u'content', u'podcast', u'page', u'needs', u'audio', u'photo', u'page', u'needs', u'images', u'blog', u'needs', u'links', u'favorite', u'music', u'iweb', u'needs', u'ilife', u'media', u'browser', u'using', u'media', u'browser', u'access', u'ilife', u'content', u'photos', u'video', u'audio', u'without', u'leaving', u'iweb', u'drag', u'podcast', u'song', u'recorded', u'garageband', u'earlier', u'today', u'iphoto', u'album', u'vacation', u'latest', u'imovie', u'project', u'whatever', u'want', u'share', u'll', u'find', u'ilife', u'media', u'browser', u'blogging', u'use', u'iweb', u'start', u'weblog', u'add', u'new', u'entries', u'easily', u'writing', u'email', u'choose', u'blog', u'template', u'type', u'text', u'drag', u'photos', u'ilife', u'media', u'browser', u'iweb', u'takes', u'care', u'everything', u'else', u'setting', u'navigation', u'blog', u'creating', u'summary', u'page', u'adding', u'entry', u'archive', u'iweb', u'also', u'handles', u'rss', u'feed', u'blog', u'anyone', u'subscribe', u're', u'done', u'adding', u'entry', u'one', u'click', u'publishes', u'blog', u'via', u'mac', u'podcasting', u'comes', u'time', u'take', u'podcasts', u'live', u'iweb', u'gives', u'simple', u'stylish', u'way', u'either', u'send', u'podcast', u'iweb', u'garageband', u'start', u'iweb', u'podcast', u'page', u'template', u'drag', u'podcast', u'ilife', u'media', u'browser', u'type', u'placeholder', u'text', u'add', u'brief', u'description', u'podcast', u'click', u'publish', u'internet', u'using', u'mac', u'account', u'iweb', u'takes', u'care', u'rss', u'feed', u'podcast', u'lets', u'submit', u'podcasts', u'itunes', u'music', u'store', u'anyone', u'listen', u'subscribe', u'one', u'click', u'mac', u'publishing', u'sharing', u'website', u'world', u'one', u'click', u'simple', u'iweb', u'mac', u'membership', u'publish', u'entire', u'website', u'complete', u'blog', u'entries', u'photo', u'albums', u'links', u'photocasts', u'movies', u'podcasts', u'internet', u'single', u'click', u'configuration', u'hassle', u'click', u'publish', u'iweb', u'automatically', u'publishes', u'entire', u'site', u'internet', u'anyone', u'web', u'browser', u'see', u'iweb', u'even', u'lets', u'announce', u'website', u'via', u'email', u'friends', u'family', u'stay', u'loop', u'ilife', u'06', u'family', u'pack', u'information', u'family', u'pack', u'software', u'license', u'agreement', u'allows', u'install', u'use', u'one', u'copy', u'apple', u'software', u'maximum', u'five', u'apple', u'labeled', u'computers', u'time', u'long', u'computers', u'located', u'household', u'used', u'persons', u'occupy', u'household', u'household', u'apple', u'means', u'person', u'persons', u'sharing', u'housing', u'unit', u'home', u'apartment', u'mobile', u'home', u'condominium', u'license', u'extend', u'students', u'reside', u'separate', u'campus', u'location', u'business', u'commercial', u'users', u'apple', u'computer'], 1520)]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "amazonCountRDD = amazonRDDtokenized.map(lambda x: (x[0], x[1], len(x[1]))).cache()\n",
    "\n",
    "smallestItem = amazonCountRDD.takeOrdered(1, lambda x: x[2])\n",
    "largestItem = amazonCountRDD.takeOrdered(1, lambda x: -x[2])\n",
    "\n",
    "print \"amazon smallest item, with length %d, is:\" % smallestItem[0][2]\n",
    "print smallestItem\n",
    "print \" \"\n",
    "\n",
    "print \"amazon largest item, with length %d, is:\" % largestItem[0][2]\n",
    "print largestItem\n",
    "print \" \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the list of tokens into a sparse structure (dictionary). This will allow us to efficiently compute scalar products between sparse vectors. \n",
    "\n",
    "#### **EXERCISE:**  Complete the definition of the \"compute_tf\" function that takes as input a list of tokens and produces a dictionary {key: value}, one key for every unique element in the list of tokens. The value for every token is the number of times that tokens appears in the list, divided by the total number of tokens. This measurement is know as **TF**, term frequency. \n",
    "\n",
    "#### **_The answer should be_:**\n",
    "\n",
    "<pre><code>\n",
    "{'sentence': 0.037037037037037035, 'text': 0.037037037037037035, 'ir': 0.037037037037037035, 'multiset': 0.037037037037037035, 'even': 0.037037037037037035, 'information': 0.037037037037037035, 'document': 0.037037037037037035, 'used': 0.037037037037037035, 'processing': 0.037037037037037035, 'grammar': 0.037037037037037035, 'words': 0.07407407407407407, 'represented': 0.037037037037037035, 'word': 0.037037037037037035, 'etrieval': 0.037037037037037035, 'keeping': 0.037037037037037035, 'natural': 0.037037037037037035, 'language': 0.037037037037037035, 'multiplicity': 0.037037037037037035, 'disregarding': 0.037037037037037035, 'bag': 0.07407407407407407, 'simplifying': 0.037037037037037035, 'representation': 0.037037037037037035, 'model': 0.07407407407407407, 'order': 0.037037037037037035}\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 0.037037037037037035, 'text': 0.037037037037037035, 'ir': 0.037037037037037035, 'multiset': 0.037037037037037035, 'even': 0.037037037037037035, 'information': 0.037037037037037035, 'document': 0.037037037037037035, 'used': 0.037037037037037035, 'processing': 0.037037037037037035, 'grammar': 0.037037037037037035, 'words': 0.07407407407407407, 'represented': 0.037037037037037035, 'word': 0.037037037037037035, 'etrieval': 0.037037037037037035, 'keeping': 0.037037037037037035, 'natural': 0.037037037037037035, 'language': 0.037037037037037035, 'multiplicity': 0.037037037037037035, 'disregarding': 0.037037037037037035, 'bag': 0.07407407407407407, 'simplifying': 0.037037037037037035, 'representation': 0.037037037037037035, 'model': 0.07407407407407407, 'order': 0.037037037037037035}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def compute_tf(tokens):\n",
    "    tf_dict = {}\n",
    "    for tf in tokens:\n",
    "        try:\n",
    "            tf_dict[tf] += 1.0\n",
    "        except:\n",
    "            tf_dict[tf] = 1.0\n",
    "            \n",
    "    Total = len(tokens)\n",
    "    for key in tf_dict.keys():\n",
    "        tf_dict[key] /= Total \n",
    "    return tf_dict\n",
    "\n",
    "print compute_tf(tokeniza(texto, stopwords)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us implement now the cosine similarity function between two items using the TF measurement.  It is defined as:\n",
    "\n",
    "#### $$ cosim(a,b) = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{\\sum a_i b_i}{\\sqrt{\\sum a_i^2} \\sqrt{\\sum b_i^2}} $$\n",
    "\n",
    "#### **EXERCISE:** We observe in the formula that we need a dot product function (between two sparse dictionaries), and a norm function.  Complete the definition of the \"dotproduct\" and \"norm\" functions below. \n",
    "\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "\n",
    "<pre><code>\n",
    "The dot product between tf_dict1 and tf_dict2 is 0.055556\n",
    "\n",
    "The norm of tf_dict1 is 0.212762\n",
    "\n",
    "The norm of tf_dict2 is 0.408248\n",
    "\n",
    "The cosim of a vector with itself must be one: 1.000000\n",
    "\n",
    "The cosim of a vector with itself must be one: 1.000000\n",
    "\n",
    "The cosim between two vectors does not depend of the order: 0.639602 = 0.639602\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product between tf_dict1 and tf_dict2 is 0.055556\n",
      "\n",
      "The norm of tf_dict1 is 0.212762\n",
      "\n",
      "The norm of tf_dict2 is 0.408248\n",
      "\n",
      "The cosim of a vector with itself must be one: 1.000000\n",
      "\n",
      "The cosim of a vector with itself must be one: 1.000000\n",
      "\n",
      "The cosim between two vectors does not depend of the order: 0.639602 = 0.639602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def dotprod(tf_dict1, tf_dict2):\n",
    "    common_tokens = list(set(tf_dict1.keys()).intersection(tf_dict2.keys()))       \n",
    "    dotProd = 0\n",
    "    for token in common_tokens:\n",
    "        dotProd += tf_dict1[token]*tf_dict2[token]\n",
    "    return dotProd\n",
    "\n",
    "def norm(tf_dict1):\n",
    "    dp = dotprod(tf_dict1, tf_dict1)\n",
    "    norma = math.sqrt(dp)\n",
    "    return norma\n",
    "\n",
    "def cosim(tf_dict1, tf_dict2):\n",
    "    dp = dotprod(tf_dict1, tf_dict2)\n",
    "    norm1 = norm(tf_dict1)\n",
    "    norm2 = norm(tf_dict2)\n",
    "    cs = dp / norm1 / norm2\n",
    "    return cs\n",
    "\n",
    "\n",
    "tf_dict1 = compute_tf(tokeniza(texto, stopwords))\n",
    "tf_dict2 = compute_tf(tokeniza(texto[0:60], stopwords))\n",
    "\n",
    "print \"The dot product between tf_dict1 and tf_dict2 is %f\\n\" % dotprod(tf_dict1, tf_dict2) \n",
    "print \"The norm of tf_dict1 is %f\\n\" % norm(tf_dict1) \n",
    "print \"The norm of tf_dict2 is %f\\n\" % norm(tf_dict2) \n",
    "print \"The cosim of a vector with itself must be one: %f\\n\" % cosim(tf_dict1, tf_dict1) \n",
    "print \"The cosim of a vector with itself must be one: %f\\n\" % cosim(tf_dict2, tf_dict2) \n",
    "print \"The cosim between two vectors does not depend of the order: %f = %f\\n\" % (cosim(tf_dict1, tf_dict2) , cosim(tf_dict2, tf_dict1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use these functions to process the datasets and find the two most similar records between amazon and google. \n",
    "\n",
    "#### **EXERCISE:** Generate a new \"allPairsRDD\" that contains all possible combinations between elements of googleSmallRDDtokenized and amazonSmallRDDtokenized. You may want to use the \"cartesian\" transformation. Transform \"allPairsRDD\" to obtain a new RDD with the format (googleID, amazonID, cosim). Finally, print the contents with the largest cosine similarity.\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "\n",
    "<pre><code>\n",
    "The allPairsRDD has 40000 elements.\n",
    "\n",
    "This is one of the elements in allPairsRDD:\n",
    "\n",
    "[(('http://www.google.com/base/feeds/snippets/11448761432933644608', ['spanish', 'vocabulary', 'builder', 'expand', 'vocabulary', 'contains', 'fun', 'lessons', 'teach', 'entertain', 'll', 'quickly', 'find', 'mastering', 'new', 'terms', 'includes', 'games']), ('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund']))]\n",
    "\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The allPairsRDD has 40000 elements.\n",
      "\n",
      "This is one of the elements in allPairsRDD:\n",
      "\n",
      "[((u'http://www.google.com/base/feeds/snippets/11448761432933644608', [u'spanish', u'vocabulary', u'builder', u'expand', u'vocabulary', u'contains', u'fun', u'lessons', u'teach', u'entertain', u'll', u'quickly', u'find', u'mastering', u'new', u'terms', u'includes', u'games']), (u'b000jz4hqo', [u'clickart', u'950', u'000', u'premier', u'image', u'pack', u'dvd', u'rom', u'broderbund']))]\n"
     ]
    }
   ],
   "source": [
    "allPairsRDD = (googleSmallRDDtokenized\n",
    "              .cartesian(amazonSmallRDDtokenized)\n",
    "              .cache())\n",
    "\n",
    "print \"The allPairsRDD has %d elements.\\n\" % allPairsRDD.count()\n",
    "\n",
    "print \"This is one of the elements in allPairsRDD:\\n\"\n",
    "print allPairsRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EXERCISE:** Transform \"allPairsRDD\" to obtain a new \"cosimRDD\" with the format (googleID, amazonID, cosim). \n",
    "\n",
    "#### **_The answer should be_:**\n",
    "\n",
    "<pre><code>\n",
    "This is the first element in cosimRDD:\n",
    "\n",
    "[('http://www.google.com/base/feeds/snippets/11448761432933644608', 'b000jz4hqo', 0.0)]\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first element in cosimRDD:\n",
      "\n",
      "[(u'http://www.google.com/base/feeds/snippets/11448761432933644608', u'b000jz4hqo', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "#print allPairsRDD.take(1)[0][0][0]\n",
    "#print allPairsRDD.take(1)[0][0][1]\n",
    "#print \"-----------\"\n",
    "#print allPairsRDD.take(1)[0][1][0]\n",
    "#print allPairsRDD.take(1)[0][1][1]\n",
    "\n",
    "cosimRDD = (allPairsRDD\n",
    "              .map(lambda x: (x[0][0], x[1][0],  cosim(compute_tf(x[0][1]) ,compute_tf(x[1][1]) )   ))\n",
    "              .cache())\n",
    "\n",
    "print \"This is the first element in cosimRDD:\\n\"\n",
    "print cosimRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EXERCISE:**  Now, find the element with the largest similarity.\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "\n",
    "<pre><code>\n",
    "This is the element in cosimRDD with the largest similarity:\n",
    "\n",
    "[('http://www.google.com/base/feeds/snippets/18411875162562199123', 'b000j4k804', 0.9712858623572642)]\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the element in cosimRDD with the largest similarity:\n",
      "\n",
      "[(u'http://www.google.com/base/feeds/snippets/18411875162562199123', u'b000j4k804', 0.9712858623572642)]\n"
     ]
    }
   ],
   "source": [
    "mostSimilarPair = (cosimRDD\n",
    "              .takeOrdered(1, lambda x: -x[2])\n",
    "              )\n",
    "\n",
    "print \"This is the element in cosimRDD with the largest similarity:\\n\"\n",
    "print mostSimilarPair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EXERCISE:**  As a final step, we will print the contents in amazon and google corresponding to that element, to check if they are similar or not.\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "\n",
    "<pre><code>\n",
    "\n",
    "The google ID is: http://www.google.com/base/feeds/snippets/18411875162562199123\n",
    "\n",
    "The amazon ID is: b000j4k804\n",
    "\n",
    "The google content is: topics entertainment 40248 instant immersion spanish audio book audio book \"instant immersion spanish (audio book) (audio book)\" \"topics entertainment\"\n",
    "\n",
    "The amazon content is: instant immersion spanish (audio book) \"instant immersion spanish (audio book) (audio book)\" \"topics entertainment\"\n",
    "\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The google ID is: http://www.google.com/base/feeds/snippets/18411875162562199123\n",
      "\n",
      "The amazon ID is: b000j4k804\n",
      "\n",
      "The google content is:\n",
      "\n",
      "topics entertainment 40248 instant immersion spanish audio book audio book \"instant immersion spanish (audio book) (audio book)\" \"topics entertainment\"\n",
      "\n",
      "The amazon content is:\n",
      "\n",
      "instant immersion spanish (audio book) \"instant immersion spanish (audio book) (audio book)\" \"topics entertainment\"\n"
     ]
    }
   ],
   "source": [
    "googleID = mostSimilarPair[0][0]\n",
    "print \"The google ID is: %s\\n\" % googleID\n",
    "amazonID = mostSimilarPair[0][1]\n",
    "print \"The amazon ID is: %s\\n\" % amazonID\n",
    "\n",
    "googleContent = (googleSmallRDD\n",
    "                         .map(lambda x: x.split(';'))\n",
    "                         .filter(lambda x: x[0] == googleID)\n",
    "                         .map(lambda x: x[1])\n",
    "                         .first()\n",
    "                )\n",
    "\n",
    "amazonContent = (amazonSmallRDD\n",
    "                         .map(lambda x: x.split(';'))\n",
    "                         .filter(lambda x: x[0] == amazonID)\n",
    "                         .map(lambda x: x[1])\n",
    "                         .first()\n",
    "                )\n",
    "\n",
    "print \"The google content is:\\n\"\n",
    "print googleContent\n",
    "print \"\\nThe amazon content is:\\n\"\n",
    "print amazonContent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EXERCISE:**  Repeat the computations with the full datasets. Open in a new browser the page localhost:4040 to see the evolution of the tasks.\n",
    "\n",
    "#### **_The answer should be_:**\n",
    "\n",
    "<pre><code>\n",
    "The allPairsRDD has 4397038 elements.\n",
    "\n",
    "This is one of the elements in allPairsRDD:\n",
    "\n",
    "[(('http://www.google.com/base/feeds/snippets/11125907881740407428', ['learning', 'quickbooks', '2007', 'learning', 'quickbooks', '2007', 'intuit']), ('b000jz4hqo', ['clickart', '950', '000', 'premier', 'image', 'pack', 'dvd', 'rom', 'broderbund']))]\n",
    "\n",
    "This is the first element in cosimRDD:\n",
    "\n",
    "[('http://www.google.com/base/feeds/snippets/11125907881740407428', 'b000jz4hqo', 0.0)]\n",
    "\n",
    "This is the element in cosimRDD with the largest similarity:\n",
    "\n",
    "[('http://www.google.com/base/feeds/snippets/17521446718236049500', 'b000v9yxj4', 1.0)]\n",
    "\n",
    "The google ID is: http://www.google.com/base/feeds/snippets/17521446718236049500\n",
    "\n",
    "The amazon ID is: b000v9yxj4\n",
    "\n",
    "The google content is: nero inc nero 8 ultra edition  \n",
    "\n",
    "The amazon content is: nero 8 ultra edition  \"nero inc.\"\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The allPairsRDD has 4397038 elements.\n",
      "\n",
      "This is one of the elements in allPairsRDD:\n",
      "\n",
      "[((u'http://www.google.com/base/feeds/snippets/11125907881740407428', [u'learning', u'quickbooks', u'2007', u'learning', u'quickbooks', u'2007', u'intuit']), (u'b000jz4hqo', [u'clickart', u'950', u'000', u'premier', u'image', u'pack', u'dvd', u'rom', u'broderbund']))]\n",
      "\n",
      "This is the first element in cosimRDD:\n",
      "\n",
      "[(u'http://www.google.com/base/feeds/snippets/11125907881740407428', u'b000jz4hqo', 0.0)]\n",
      "\n",
      "This is the element in cosimRDD with the largest similarity:\n",
      "\n",
      "[(u'http://www.google.com/base/feeds/snippets/17521446718236049500', u'b000v9yxj4', 1.0)]\n",
      "The google ID is: http://www.google.com/base/feeds/snippets/17521446718236049500\n",
      "\n",
      "The amazon ID is: b000v9yxj4\n",
      "\n",
      "The google content is: nero inc nero 8 ultra edition  \n",
      "\n",
      "The amazon content is: nero 8 ultra edition  \"nero inc.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allPairsRDD = (googleRDDtokenized\n",
    "              .cartesian(amazonRDDtokenized)\n",
    "              .cache())\n",
    "\n",
    "print \"The allPairsRDD has %d elements.\\n\" % allPairsRDD.count()\n",
    "\n",
    "print \"This is one of the elements in allPairsRDD:\\n\"\n",
    "print allPairsRDD.take(1)\n",
    "\n",
    "cosimRDD = (allPairsRDD\n",
    "              .map(lambda x: (x[0][0], x[1][0],  cosim(compute_tf(x[0][1]) ,compute_tf(x[1][1]) )   ))\n",
    "              .cache())\n",
    "\n",
    "print \"\\nThis is the first element in cosimRDD:\\n\"\n",
    "print cosimRDD.take(1)\n",
    "\n",
    "mostSimilarPair = (cosimRDD\n",
    "              .takeOrdered(1, lambda x: -x[2])\n",
    "              )\n",
    "\n",
    "print \"\\nThis is the element in cosimRDD with the largest similarity:\\n\"\n",
    "print mostSimilarPair\n",
    "\n",
    "googleID = mostSimilarPair[0][0]\n",
    "print \"The google ID is: %s\\n\" % googleID\n",
    "amazonID = mostSimilarPair[0][1]\n",
    "print \"The amazon ID is: %s\\n\" % amazonID\n",
    "\n",
    "googleContent = (googleRDD\n",
    "                         .map(lambda x: x.split(';'))\n",
    "                         .filter(lambda x: x[0] == googleID)\n",
    "                         .map(lambda x: x[1])\n",
    "                         .first()\n",
    "                )\n",
    "\n",
    "amazonContent = (amazonRDD\n",
    "                         .map(lambda x: x.split(';'))\n",
    "                         .filter(lambda x: x[0] == amazonID)\n",
    "                         .map(lambda x: x[1])\n",
    "                         .first()\n",
    "                )\n",
    "\n",
    "print \"The google content is: %s\\n\" % googleContent\n",
    "print \"The amazon content is: %s\\n\" % amazonContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
