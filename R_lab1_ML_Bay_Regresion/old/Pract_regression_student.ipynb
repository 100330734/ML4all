{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian and Gaussian Process regression\n",
    "\n",
    "    Notebook version: 1.1 (Sep 29, 2017)\n",
    "\n",
    "    Authors: Miguel Lázaro Gredilla\n",
    "             Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "             Jesús Cid Sueiro (jesus.cid@uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "    Changes: v.1.0 - First version. Python version\n",
    "             v.1.1 - Python 3 compatibility. ML section.\n",
    "    \n",
    "    Pending changes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io       # To read matlab files\n",
    "from scipy import spatial\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = 8, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this exercise the student will review several key concepts of Bayesian regression and Gaussian processes.\n",
    "\n",
    "For the purpose of this exercise, the regression model is\n",
    "\n",
    "$${s}({\\bf x}) = f({\\bf x}) + \\varepsilon$$\n",
    "\n",
    "where ${s}({\\bf x})$ is the output corresponding to input ${\\bf x}$, $f({\\bf x})$ is the unobservable latent function, and $\\varepsilon$ is white zero-mean Gaussian noise, i.e., $\\varepsilon \\sim {\\cal N}(0,\\sigma_\\varepsilon^2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical considerations\n",
    "\n",
    "   - Though sometimes unavoidable, it is recommended not to use explicit matrix inversion whenever possible. For instance, if an operation like ${\\mathbf A}^{-1} {\\mathbf b}$ must be performed, it is preferable to code it using python $\\mbox{numpy.linalg.lstsq}$ function (see http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html), which provides the LS solution to the overdetermined system ${\\mathbf A} {\\mathbf w} = {\\mathbf b}$.\n",
    "   \n",
    "   - Sometimes, the computation of $\\log|{\\mathbf A}|$ (where ${\\mathbf A}$ is a positive definite matrix) can overflow available precision, producing incorrect results. A numerically more stable alternative, providing the same result is $2\\sum_i \\log([{\\mathbf L}]_{ii})$, where $\\mathbf L$ is the Cholesky decomposition of $\\mathbf A$ (i.e., ${\\mathbf A} = {\\mathbf L}^\\top {\\mathbf L}$), and $[{\\mathbf L}]_{ii}$ is the $i$th element of the diagonal of ${\\mathbf L}$.\n",
    "   \n",
    "   - Non-degenerate covariance matrices, such as the ones in this exercise, are always positive definite. It may happen, as a consequence of chained rounding errors, that a matrix which was mathematically expected to be positive definite, turns out not to be so. This implies its Cholesky decomposition will not be available. A quick way to palliate this problem is by adding a small number (such as $10^{-6}$) to the diagonal of such matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility of computations\n",
    "\n",
    "To guarantee the exact reproducibility of the experiments, it may be useful to start your code initializing the seed of the random numbers generator, so that you can compare your results with the ones given in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data generation with a linear model\n",
    "\n",
    "During this section, we will assume the following parametric model for the latent function\n",
    "\n",
    "$$f({\\bf x}) = {\\bf w}^\\top {\\bf z}$$\n",
    "\n",
    "where ${\\bf z} = (1, {\\bf x}^\\top)^\\top$. This is a linear model in the observations, where ${\\bf w}$ contains the parameters of the model. The <i>a priori</i> distribution of ${\\bf w}$ is assumed to be\n",
    "\n",
    "$${\\bf w} \\sim {\\cal N}({\\bf 0}, \\sigma_p^2~{\\bf I})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Synthetic data generation\n",
    "\n",
    "First, we are going to generate synthetic data (so that we have the ground-truth model) and use them to make sure everything works correctly and our estimations are sensible.\n",
    "\n",
    "* [1] Set parameters $\\sigma_p^2 = 2$ and $\\sigma_{\\varepsilon}^2 = 0.2$. To do so, define variables `sigma_p` and `sigma_eps` containing the respectiv standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter settings\n",
    "# sigma_p = <FILL IN>\n",
    "# sigma_eps = <FILL IN>\n",
" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [2] Generate a weight vector $\\mbox{true_w}$ with two elements from the <i>a priori</i> distribution of the weights. This vector determines the regression line that we want to find (i.e., the optimum unknown solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data dimension:\n",
    "dim_x = 2\n",
    "\n",
    "# Generate a parameter vector taking a random sample from the prior distributions\n",
    "# (the np.random module may be usefull for this purpose)\n",
    "# true_w = <FILL IN>\n",
    "\n",
    "print('The true parameter vector is {0}'.format(true_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [3] Generate an input matrix $\\mbox{X}$ containing 20 samples equally spaced values between 0 and 2 in the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [4] Finally, generate the output vector ${\\mbox s}$ as the product $\\mbox{X} \\ast \\mbox{true_w}$ plus Gaussian noise of pdf ${\\cal N}(0,\\sigma_\\varepsilon^2)$ at each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expand input matrix with an all-ones column\n",
    "col_1 = np.ones((n_points,))\n",
    "# Xe = <FILL IN>\n",
    "\n",
    "# Generate values of the target variable\n",
    "# s = <FILL IN>\n",
" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the generated data. You will notice a linear behavior, but the presence of noise makes it hard to estimate precisely the original straight line that generated them (which is stored in $\\mbox{true_w}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL> \n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Maximum Likelihood (ML) regression\n",
    "\n",
    "### 3.1. Likelihood function\n",
    "\n",
    "\n",
    "i.e., a linear model in the observations, where ${\\bf w}$ contains the parameters of the model. The <i>a priori</i> distribution of ${\\bf w}$ is assumed to be\n",
    "\n",
    "$${\\bf w} \\sim {\\cal N}({\\bf 0}, \\sigma_p^2~{\\bf I})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] Define a function `predict(we, Xe)` that computes the linear predictions for all inputs in data matrix `Xe` (a 2-D numpy arry), for a given parameter vector `we` (a 1-D numpy array). The output should be a 1-D array. Test your function with the given dataset and `we = [0.4, 0.7]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>\n",
    "\n",
    "# Print predictions\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [2] Define a function `sse(we, X, s)` that computes the sum of squared errors (SSE) for the linear prediction with parameters `we ` (1D numpy array), inputs `Xe `  (2D numpy array) and targets `s ` (1D numpy array). Using this function, compute the SSE of the true parameter vector in `true_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>\n",
    "\n",
    "print(\" The SSE is: {0}\".format(SSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [3] Define a function `likelihood(we, Xe, s)` that computes the likelihood of parameter vector `we` for a given dataset in matrix `Xe` and vector `s`. Note that this function can use the `sse` function defined above. Using this function, compute the likelihood of the true parameter vector in `true_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>\n",
    "\n",
    "print(\"The likelihood of the true parameter vector is {0}\".format(L_w_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [4] In order to visualize the likelihood function, generate a set of points in a two dimensional grid going from $(-\\sigma_p, -\\sigma_p)$ to $(\\sigma_p, \\sigma_p)$, compute the likelihood for all these points and visualize them using a 2-dimensional plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First construct a grid of (theta0, theta1) parameter pairs and their\n",
    "# corresponding cost function values.\n",
    "N = 200    # Number of points along each dimension.\n",
    "# w0_grid = np.linspace(<FILL IN>)\n",
    "# w1_grid = np.linspace(<FILL IN>)\n",
    "\n",
    "\n",
    "Lw = np.zeros((N,N))\n",
    "# Fill Lw with the likelihood values\n",
    "# < write  your code here>\n",
    "# <SOL>\n",

    "# </SOL>\n",
    "\n",
    "WW0, WW1 = np.meshgrid(w0_grid, w1_grid, indexing='ij')\n",
    "contours = plt.contour(WW0, WW1, Lw, 50)\n",
    "\n",
    "plt.figure\n",
    "plt.clabel(contours)\n",
    "plt.scatter([true_w[0]]*2, [true_w[1]]*2, s=[50,10], color=['k','w'])\n",
    "plt.xlabel('$w_0$')\n",
    "plt.ylabel('$w_1$')\n",
    "\n",
    "print(true_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. ML estimate\n",
    "\n",
    "* [1] Compute the ML estimate of $w_e$ given the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>\n",
    "\n",
    "print(w_ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [2] Compute the maximum likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>\n",
    "\n",
    "print('Maximum likelihood: {0}'.format(L_w_ML))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Convergence of the ML estimate for the true model\n",
    "\n",
    "Note that the likelihood of the true parameter vector is, in general, smaller than that of the ML estimate. However, as the sample size increasis, both should converge to the same value.\n",
    "\n",
    "* [1] Generate a longer dataset, with $K_\\text{max}=2^{16}$ samples, uniformly spaces between 0 and 2. Store it in the 2D-array `X2` and the 1D-array `s2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter settings\n",
    "x_min = 0\n",
    "x_max = 2\n",
    "n_points = 2**16\n",
    "\n",
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [2] Compute the ML estimate based on the first $2^k$ samples, for $k=2,3,\\ldots, 16$. For each value of $k$ compute the squared euclidean distance between the true parameter vector and the ML estimate. Represent it graphically (using a logarithmic scale in the y-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Posterior pdf of the weight vector\n",
    "\n",
    "In this section we will visualize prior and the posterior distribution functions. First, we will restore the dataset at the begining of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "n_points = 21\n",
    "true_w = sigma_p * np.random.randn(dim_x)\n",
    "X = np.linspace(x_min, x_max, n_points)\n",
    "col_1 = np.ones((n_points,))\n",
    "Xe = np.vstack((col_1, X)).T\n",
    "s = Xe.dot(true_w) + sigma_eps * np.random.randn(n_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] Define a function `posterior_stats(Xe, s)` that computes the parameters of the posterior coefficient distribution given the dataset in matrix `Xe` and vector `s`. This function should return the posterior mean, the covariance matrix and the precision matrix (the inverse of the covariance matrix). Test the function to the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>\n",
    "\n",
    "mean_w, Cov_w, iCov_w = posterior_stats(Xe, s)\n",
    "\n",
    "print('true_w = {0}'.format(true_w))\n",
    "print('mean_w = {0}'.format(mean_w))\n",
    "print('Cov_w = {0}'.format(Cov_w))\n",
    "print('iCov_w = {0}'.format(iCov_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [2] Define a function `gauss_pdf(we, mean_w, iCov_w)` that computes the Gaussian pdf with mean `mean_w` and precision matrix `iCov_w`. Use this function to compute and compare the posterior pdf value of the true coefficients, the ML estimate and the MSE estimate, given the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>\n",
    "\n",
    "print('p(true_w | s)  = {0}'.format(gauss_pdf(true_w, mean_w, iCov_w)))\n",
    "print('p(w_ML | s)  = {0}'.format(gauss_pdf(w_ML, mean_w, iCov_w)))\n",
    "print('p(w_MSE | s)  = {0}'.format(gauss_pdf(mean_w, mean_w, iCov_w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [3] The function `computePW` computes the posterior distribution over a grid of points. Use this function to compute a bidimensional plot of the prior and the posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def computePW(N, lims, mean_w, iCov_w):\n",
    "\n",
    "    # First construct a grid of (theta0, theta1) parameter pairs and their\n",
    "    # corresponding cost function values.\n",
    "    w0_grid = np.linspace(lims[0], lims[1], N)\n",
    "    w1_grid = np.linspace(lims[2], lims[3], N)\n",
    "\n",
    "    Pw = np.zeros((N,N))\n",
    "    for i, w0i in enumerate(w0_grid):\n",
    "        for j, w1j in enumerate(w1_grid):\n",
    "            we = np.array((w0i, w1j)) \n",
    "            Pw[i, j] = gauss_pdf(we, mean_w, iCov_w)\n",
    "\n",
    "    WW0, WW1 = np.meshgrid(w0_grid, w1_grid, indexing='ij')\n",
    "\n",
    "    return Pw, WW0, WW1\n",
    "\n",
    "# Common parameters for all plots\n",
    "lims = sigma_p*np.array([-3, 3, -1.5, 1.5])\n",
    "N = 200\n",
    "\n",
    "# Define figure with two subplots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15, 4))\n",
    "\n",
    "# Compute the values of the prior distribution\n",
    "# Pw, WW0, WW1 = computePW(<FILL IN>)\n",
    "contours = ax1.contour(WW0, WW1, Pw, 40)\n",
    "ax1.clabel(contours)\n",
    "ax1.set_xlabel('$w_0$')\n",
    "ax1.set_ylabel('$w_1$')\n",
    "ax1.axis('equal')\n",
    "ax1.grid('on')\n",
    "\n",
    "# Compute the values of the posterior distribution\n",
    "# Pw, WW0, WW1 = computePW(<FILL IN>)\n",
    "contours = ax2.contour(WW0, WW1, Pw, 40)\n",
    "ax2.clabel(contours)\n",
    "ax2.set_xlabel('$w_0$')\n",
    "ax2.set_ylabel('$w_1$')\n",
    "ax2.axis('equal')\n",
    "ax2.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Sampling regression curves from the posterior\n",
    "\n",
    "In this section we will plot the functions corresponding to different samples drawn from the posterior distribution of the weight vector. \n",
    "\n",
    "To this end, we will first generate an input dataset of equally spaced samples. We will compute the functions at these points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition of the interval for representation purposes \n",
    "x2_min = -1\n",
    "x2_max = 3\n",
    "n_points = 100   # Only two points are needed to plot a straigh line\n",
    "\n",
    "# Build the input data matrix:\n",
    "# Input values for representation of the regression curves\n",
    "X2 = np.linspace(x2_min, x2_max, n_points)\n",
    "col_1 = np.ones((n_points,))\n",
    "X2e = np.vstack((col_1, X2)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random vectors ${\\bf w}_l$ with $l = 1,\\dots, 50$, from the posterior density of the weights, $p({\\bf w}\\mid{\\bf s})$, and use them to generate 50 straight lines, $f({\\bf x}^\\ast) = {{\\bf x}^\\ast}^\\top {\\bf w}_l$, with the second component of ${\\bf x}^\\ast$ between $-1$ and $3$, with step $0.1$.\n",
    "\n",
    "Plot the original ground-truth straight line, corresponding to $\\mbox{true_w}$, along with the $50$ generated straight lines and the original samples, all in the same plot. As you can check, the Bayesian model is not providing a single answer, but instead a density over them, from which we have extracted 50 options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drawing weights from the posterior\n",
    "# First, compute the cholesky decomposition of the covariance matrix\n",
    "# L = <FILL IN>\n",
    "\n",
    "for l in range(50):\n",
    "    # Generate a random sample from the posterior distribution\n",
    "    # w_l = <FILL IN>\n",
    "\n",
    "    # Compute predictions for the inputs in the data matrix\n",
    "    # p_l = <FILL IN>\n",
    "\n",
    "    # Plot prediction function\n",
    "    # plt.plot(<FILL IN>, 'c:');\n",
    "\n",
    "# Compute predictions for the inputs in the data matrix and using the true model\n",
    "# p_truew = <FILL IN>\n",
    "\n",
    "# Plot the true model\n",
    "plt.plot(X2, p_truew, 'b', label='True model', linewidth=2);\n",
    "\n",
    "# Plot the training points\n",
    "plt.plot(X,s,'r.',markersize=12);\n",
    "\n",
    "plt.xlim((x2_min,x2_max));\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('$x$',fontsize=14);\n",
    "plt.ylabel('$s$',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Plotting the confidence intervals\n",
    "\n",
    "On top of the previous figure (copy here your code from the previous section), plot functions\n",
    "\n",
    "$${\\mathbb E}\\left\\{f({\\bf x}^\\ast)\\mid{\\bf s}\\right\\}$$\n",
    "\n",
    "and\n",
    "\n",
    "$${\\mathbb E}\\left\\{f({\\bf x}^\\ast)\\mid{\\bf s}\\right\\} \\pm 2 \\sqrt{{\\mathbb V}\\left\\{f({\\bf x}^\\ast)\\mid{\\bf s}\\right\\}}$$\n",
    "\n",
    "(i.e., the posterior mean of $f({\\bf x}^\\ast)$, as well as two standard deviations above and below).\n",
    "\n",
    "It is possible to show analytically that this region comprises $95.45\\%$ probability of the posterior probability $p(f({\\bf x}^\\ast)\\mid {\\bf s})$ at each ${\\bf x}^\\ast$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that you can re-use code from sect. 4.2 to solve this exercise\n",
    "\n",
    "# Plot sample functions from the posterior, and the training points\n",
    "# <SOL>\n",

    "# </SOL>    \n",
    "\n",
    "# Plot the posterior mean.\n",
    "# mean_ast = <FILL IN>\n",
    "plt.plot(X2, mean_ast, 'm', label='Predictive mean', linewidth=2);\n",
    "\n",
    "# Plot the posterior mean \\pm 2 std\n",
    "# std_ast = <FILL IN>\n",
    "# plt.plot(<FILL IN>, 'm--', label='Predictive mean $\\pm$ 2std', linewidth=2);\n",
    "# plt.plot(<FILL IN>, 'm--', linewidth=3);\n",
    "\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('$x$',fontsize=14);\n",
    "plt.ylabel('$s$',fontsize=14);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot now ${\\mathbb E}\\left\\{s({\\bf x}^\\ast)\\mid{\\bf s}\\right\\} \\pm 2 \\sqrt{{\\mathbb V}\\left\\{s({\\bf x}^\\ast)\\mid{\\bf s}\\right\\}}$ (note that the posterior means of $f({\\bf x}^\\ast)$ and $s({\\bf x}^\\ast)$ are the same, so there is no need to plot it again). Notice that $95.45\\%$ of observed data lie now within the newly designated region. These new limits establish a confidence range for our predictions. See how the uncertainty grows as we move away from the interpolation region to the extrapolation areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot sample functions confidence intervals and sampling points\n",
    "# Note that you can simply copy and paste most of the code used in the cell above.\n",
    "\n",
    "# <SOL>\n",

    "# </SOL>    \n",
    "\n",
    "# Compute the standad deviations for s and plot the confidence intervals\n",
    "# <SOL>\n",

    "# </SOL>\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('$x$',fontsize=14);\n",
    "plt.ylabel('$s$',fontsize=14);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bayesian Inference with real data. The stocks dataset.\n",
    "\n",
    "\n",
    "Once our code has been tested on synthetic data, we will use it with real data. \n",
    "\n",
    "* [1] Load data corresponding to the evolution of the stocks of 10 airline companies. This data set is an adaptation of the Stock dataset from http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html, which in turn was taken from the StatLib Repository, http://lib.stat.cmu.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [2] Normalize the data so all training sample components have zero mean and unit standard deviation. Store the normalized training and test samples in 2D numpy arrays `Xtrain` and `Xtest`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [3] Add an all-ones column to the input data. Store the resulting 2D arrays in variables `Ztrain` and `Ztest`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this code, you will have inside matrix `Ztrain` an initial column of ones and the evolution of (normalized) price for 9 airlines, whereas vecto `Ytrain` will contain a single column with the price evolution of the tenth airline. The objective of the regression task is to estimate the price of the tenth airline from the prices of the other nine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Hyperparameter selection\n",
    "\n",
    "Since the values $\\sigma_p$ and $\\sigma_\\varepsilon$ are no longer known, a first rough estimation is needed (we will soon see how to estimate these values in a principled way).\n",
    "\n",
    "To this end, we will adjust them using the LS solution to the regression problem:\n",
    "\n",
    "   - $\\sigma_p^2$ will be taken as the average of the square values of ${\\hat {\\bf w}}_{LS}$\n",
    "   - $\\sigma_\\varepsilon^2$ will be taken as two times the average of the square of the residuals when using ${\\hat {\\bf w}}_{LS}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w_LS, residuals, rank, s = <FILL IN>\n",
    "# sigma_p = <FILL IN>\n",
    "# sigma_eps = <FILL IN>\n",
" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Posterior pdf of the weight vector\n",
    "\n",
    "Using the previous values for the hyperparameters, compute the <i>a posteriori</i> mean and covariance matrix of the weight vector ${\\bf w}$. Instead of two weights there will now be 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting posterior is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('mean_w = {0}'.format(mean_w))\n",
    "print('Cov_w = {0}'.format(Cov_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Model assessment\n",
    "\n",
    "In order to verify the performance of the resulting model, compute the posterior mean and variance of each of the test outputs from the posterior over ${\\bf w}$. I.e, compute ${\\mathbb E}\\left\\{s({\\bf x}^\\ast)\\mid{\\bf s}\\right\\}$ and $\\sqrt{{\\mathbb V}\\left\\{s({\\bf x}^\\ast)\\mid{\\bf s}\\right\\}}$ for each test sample ${\\bf x}^\\ast$ contained in each row of `Xtest`. Be sure not to use the outputs `Ytest` at any point during this process.\n",
    "\n",
    "Store the predictive mean and variance of all test samples in two column vectors called `m_y` and `v_y`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute now the mean square error (MSE) and the negative log-predictive density (NLPD) with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",

    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('MSE = {0}'.format(MSE))\n",
    "print('NLPD = {0}'.format(NLPD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two measures reveal the quality of our predictor (with lower values revealing higher quality). The first measure (MSE) only compares the predictive mean with the actual value and always has a positive value (if zero was reached, it would mean a perfect prediction). It does not take into account predictive variance. The second measure (NLPD) takes into account both the deviation and the predictive variance (uncertainty) to measure the quality of the probabilistic prediction (a high error in a prediction that was already known to have high variance has a smaller penalty, but also, announcing a high variance when the prediction error is small won’t award such a good score)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
