{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parametric Model-Based regression\n",
    "\n",
    "    Notebook version: 1.0 (Sep 23, 2016)\n",
    "\n",
    "    Author: Jesús Cid-Sueiro (jesus.cid@uc3m.es)\n",
    "            Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "\n",
    "    Changes: v.1.0 - First version, expanding some cells from the Bayesian Regression notebook\n",
    "    \n",
    "    Pending changes: * Include regression on the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io       # To read matlab files\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Model-based parametric regression\n",
    "\n",
    "### 1.1. The regression problem\n",
    "\n",
    "Given an observation vector ${\\bf x}$, the goal of the regression problem is to find a function $f({\\bf x})$ providing *good* predictions about some unknown variable $s$. To do so, we assume that a set of *labelled* training examples, $\\{{\\bf x}^{(k)}, s^{(k)}\\}_{k=1}^K$ is available. \n",
    "\n",
    "The predictor function should make good predictions for new observations ${\\bf x}$ not used during training. In practice, this is tested using a second set (the *test set*) of labelled samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. The i.i.d. assumption\n",
    "\n",
    "Most regression algorithms are grounded on the idea that all samples from the training and test sets have been generated independently by some common stochastic process. This is the reason why a model adjusted with the training data can make good predictions over new test samples. \n",
    "\n",
    "Mathemattically, this means that all pairs $({\\bf x}^{(k)}, s^{(k)})$ from the training and test sets are independent and identically distributed (i.i.d.) samples from some distribution $p_{{\\bf X}, S}({\\bf x}, s)$. Unfortunately, this distribution is generally unknown.\n",
    "\n",
    "<img src=\"figs/DataModel.png\", width=180>\n",
    "\n",
    "NOTE: In the following, we will use capital letters, ${\\bf X}$, $S$, ..., to denote random variables, and lower-case letters ${\\bf x}$, s, ..., to the denote the values they can take. When there is no ambigüity, we will remove subindices of the density functions, $p_{{\\bf X}, S}({\\bf x}, s)= p({\\bf x}, s)$ to simplify the mathematical notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3. Model-based regression\n",
    "\n",
    "If $p({\\bf x}, s)$ were know, we could apply estimation theory to estimate $s$ from $p$. For instance, we could apply any of the following classical estimates:\n",
    "\n",
    "* Maximum A Posterior (MAP): $$\\hat{s}_{\\text{MAP}} = \\arg\\max_s p(s| {\\bf x})$$\n",
    "* Minimum Mean Square Error (MSE): $$\\hat{s}_{\\text{MSE}} = \\mathbb{E}\\{S |{\\bf x}\\}$$\n",
    "\n",
    "Note that, since these estimators depend on $p(s |{\\bf x})$, knowing the posterior distribution of the target variable is enough, and we do not need to know the joint distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Model based-regression method exploit the idea of using the training data to estimate the posterior distribution $p(s|{\\bf x})$ and then apply estimation theory to make predictions.\n",
    "\n",
    "<img src=\"figs/ModelBasedReg.png\", width=280>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.4. Parametric model-based regression\n",
    "\n",
    "How can we estimate the posterior probability function of the target variable $s$? In this section we will explore a parametric estimation method: let us assume that $p$ belongs to a parametric family of distributions $p(s|{\\bf x},{\\bf w})$, where ${\\bf w}$ is some unknown parameter. We will use the training data to estimate ${\\bf w}$\n",
    "\n",
    "<img src=\"figs/ParametricReg.png\", width=300>\n",
    "\n",
    "The estimation of ${\\bf w}$ from a given dataset $\\mathcal{D}$ is the goal of the following sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.5. Maximum Likelihood parameter estimation.\n",
    "\n",
    "The ML (Maximum Likelihood) principle is well-known in statistics and can be stated as follows: take the value of the parameter to be estimated (in our case, ${\\bf w}$) that best explains the given observations (in our case, the training dataset $\\mathcal{D}$. Mathematically, this can be expressed as follows:\n",
    "\n",
    "$$\\hat{\\bf w}_{\\text{ML}} = \\arg \\max_{\\bf w} p(\\mathcal{D}|{\\bf w})$$\n",
    "\n",
    "To be more specific: let us group the target variables into a vector\n",
    "$${\\bf s} = \\left(s^{(1)}, \\dots, s^{(K)}\\right)^\\top$$\n",
    "and the input vectors into a matrix\n",
    "$${\\bf X} = \\left({\\bf x}^{(1)}, \\dots, {\\bf x}^{(K)}\\right)^\\top$$\n",
    "\n",
    "Then we can write \n",
    "\n",
    "$$p(\\mathcal{D}|{\\bf w}) \n",
    "    = p({\\bf s}, {\\bf X}|{\\bf w}) \n",
    "    = p({\\bf s} | {\\bf X} {\\bf w}) p({\\bf X}|{\\bf w})\n",
    "    = p({\\bf s} | {\\bf X} {\\bf w}) p({\\bf X})\n",
    "$$\n",
    "(where, in the last step, we have used the fact that the inputs do not depend on ${\\bf w}$, which is a parameter of the posterior distribution of $s$).\n",
    "\n",
    "Thus, we can express the estimation problem in\n",
    "\n",
    "$$\\hat{\\bf w}_{\\text{ML}} = \\arg \\max_{\\bf w} p({\\bf s}|{\\bf X},{\\bf w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Summary.\n",
    "\n",
    "Let's summarize what we need to do in order to design a regression algorithm:\n",
    "\n",
    "1. Assume a parametric data model $p(s| {\\bf x},{\\bf w})$\n",
    "2. Using the data model and the i.i.d. assumption, compute $p({\\bf s}| {\\bf X},{\\bf w})$.\n",
    "3. Find an expression for ${\\bf w}_{\\text{ML}}$\n",
    "4. Assuming ${\\bf w} = {\\bf w}_{\\text{ML}}$, compute the MAP or the minimum MSE estimate of $s$ given ${\\bf x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ML estimation for a Gaussian model.\n",
    "\n",
    "### 2.1. Step 1: The Gaussian generative model\n",
    "\n",
    "Let us assume that the target variables $s^{(k)}$ in dataset $\\mathcal{D}$ are given by\n",
    "\n",
    "$$s^{(k)} = {\\bf w}^\\top {\\bf z}^{(k)} + \\varepsilon^{(k)}$$\n",
    "\n",
    "where ${\\bf z}^{(k)}$ is the result of some transformation of the inputs, ${\\bf z}^{(k)} = T({\\bf x}^{(k)})$, and $\\varepsilon^{(k)}$ are i.i.d. instances of a Gaussian random variable with mean zero and varianze $\\sigma_\\varepsilon^2$, i.e.,\n",
    "\n",
    "$$p_E(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "                   \\exp\\left(-\\frac{\\varepsilon^2}{2\\sigma_\\varepsilon^2}\\right)$$\n",
    "\n",
    "Assuming that the noise variables are independent on ${\\bf x}$ and ${\\bf w}$, then, for a given ${\\bf x}$ and ${\\bf w}$, the target variable is gaussian with mean ${\\bf w}^\\top {\\bf z}^{(k)}$ and variance $\\varepsilon^2$\n",
    "\n",
    "$$p(s|{\\bf x}, {\\bf w}) = p_E(s-{\\bf w}^\\top{\\bf z}) =\n",
    "    \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "    \\exp\\left(-\\frac{(s-{\\bf w}^\\top{\\bf z})^2}{2\\sigma_\\varepsilon^2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Step 2: Likelihood function\n",
    "\n",
    "Now we need to compute the likelihood function $p({\\bf s}, {\\bf X} | {\\bf w})$. If the samples are i.i.d. we can write\n",
    "$$\n",
    "p({\\bf s}| {\\bf X}, {\\bf w})\n",
    "    = \\prod_{k=1}^{K} p(s^{(k)}| {\\bf x}^{(k)}, {\\bf w}) \n",
    "    = \\prod_{k=1}^{K} \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "    \\exp\\left(-\\frac{\\left(s^{(k)}-{\\bf w}^\\top{\\bf z}^{(k)}\\right)^2}{2\\sigma_\\varepsilon^2}\\right) \\\\\n",
    "    = \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\\right)^K\n",
    "      \\exp\\left(-\\sum_{k=1}^K \\frac{\\left(s^{(k)}-{\\bf w}^\\top{\\bf z}^{(k)}\\right)^2}{2\\sigma_\\varepsilon^2}\\right) \\\\\n",
    "$$\n",
    "Finally, grouping variables ${\\bf z}^{(k)}$ in\n",
    "$${\\bf Z} = \\left({\\bf z}^{(1)}, \\dots, {\\bf z}^{(K)}\\right)^\\top$$\n",
    "we get\n",
    "$$\n",
    "p({\\bf s}| {\\bf X}, {\\bf w})\n",
    "    = \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\\right)^K\n",
    "      \\exp\\left(-\\frac{1}{2\\sigma_\\varepsilon^2}\\|{\\bf s}-{\\bf Z}{\\bf w}\\|^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Step 3: ML estimation.\n",
    "\n",
    "The <b>maximum likelihood</b> solution is then given by:\n",
    "$$\n",
    "{\\bf w}_{ML} = \\arg \\max_{\\bf w} p({\\bf s}|{\\bf w}) = \\arg \\min_{\\bf w} \\|{\\bf s} - {\\bf Z}{\\bf w}\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is exactly the same optimization problem of the Least Squares (LS) regression algorithm. The solution is thus\n",
    "$$\n",
    "{\\bf w}_{ML} = ({\\bf Z}^\\top{\\bf Z})^{-1}{\\bf Z}^\\top{\\bf s}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1:\n",
    "\n",
    "Assume that the targets in the one-dimensional dataset given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([0.15, 0.41, 0.53, 0.80, 0.89, 0.92, 0.95]) \n",
    "s = np.array([0.09, 0.16, 0.63, 0.44, 0.55, 0.82, 0.95]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "have been generated by a linear Gaussian model (i.e., with $z = T(x) = x$) with noise variance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sigma_eps = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **1.1.** Represent a scatter plot of the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **1.2.** Compute the ML estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **1.3.** Plot the likelihood as a function of parameter $w$ along the interval $-0.5\\le w \\le 2$, verifying that the ML estimate takes the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "K = len(s)\n",
    "wGrid = # <FILL IN>\n",
    "\n",
    "p = []\n",
    "for w in wGrid:\n",
    "    d = s - X*w\n",
    "    p.append(# <FILL IN>)\n",
    "\n",
    "d = s-X*wML\n",
    "pML = [# <FILL IN>]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(wGrid, p)\n",
    "plt.stem(wML, pML)\n",
    "plt.xlabel('$w$')\n",
    "plt.ylabel('Likelihood function')\n",
    "plt.show()\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **1.4.** Plot the prediction function over the data scatter plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "x = np.arange(0, 1.2, 0.01)\n",
    "sML = # <FILL IN>\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, s)\n",
    "plt.plot(# <FILL IN>)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:\n",
    "\n",
    "Assume the dataset $\\mathcal{D} = \\{(x^{(k)}, s^{(k)}, k=1,\\ldots, K\\}$ contains i.i.d. samples from a distribution with posterior density given by\n",
    "$$\n",
    "p(s|x, w) = w x \\exp(- w x s), \\qquad s\\ge0, \\,\\, x\\ge 0, \\,\\, w\\ge 0\n",
    "$$\n",
    "\n",
    "* **2.1.** Determine an expression for the likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2.2.** Draw the likelihood function for the dataset in **Exercise 1** in the range $0\\le w\\le 6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "K = len(s)\n",
    "wGrid = np.arange(# <FILL IN>)\n",
    "\n",
    "p = []\n",
    "Px = np.prod(X)\n",
    "xs = np.dot(X,s)\n",
    "\n",
    "for w in wGrid:\n",
    "    p.append( # <FILL IN>)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(# <FILL IN>)\n",
    "plt.xlabel('$w$')\n",
    "plt.ylabel('Likelihood function')\n",
    "plt.show()\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2.3.** Determine the coefficient $w_\\text{ML}$ of the linear prediction function (i.e., using ${\\bf Z}={\\bf X}$). \n",
    "(*Hint: you can maximize the log of the likelihood function instead of the likelihood function in order to simplify the differentiation*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2.4.** Compute $w_\\text{ML}$ for the dataset in **Exercise 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wML =  # <FILL IN>\n",
    "print wML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.4. Multiple explanations of the data\n",
    "\n",
    "With an additive Gaussian independent noise model, the maximum likelihood and the least squares solutions are the same. We have not improved much ...\n",
    "\n",
    "However, we have already formulated the problem in a probabilistic way. This opens the door to reasoning in terms of a set of possible explanations, not just one. We believe more than one of our models could have generated the data.\n",
    "\n",
    "   - We do not believe all models are equally likely to have generated the data\n",
    "   - We may <b>believe</b> that a simpler model is more likely than a complex one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
