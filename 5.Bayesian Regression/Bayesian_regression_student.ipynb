{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Parametric Regression\n",
    "\n",
    "    Notebook version: 1.3 (Sep 26, 2016)\n",
    "\n",
    "    Author: Jerónimo Arenas García (jarenas@tsc.uc3m.es)\n",
    "            Jesús Cid-Sueiro (jesus.cid@uc3m.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "    Changes: v.1.0 - First version\n",
    "             v.1.1 - ML Model selection included\n",
    "             v.1.2 - Some typos corrected\n",
    "             v.1.3 - Rewriting text, reorganizing content, some exercises.\n",
    "    \n",
    "    Pending changes: * Include regression on the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "\n",
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "from IPython import display\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io       # To read matlab files\n",
    "import pylab\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Model-based parametric regression\n",
    "\n",
    "### 1.1. The regression problem.\n",
    "\n",
    "Given an observation vector ${\\bf x}$, the goal of the regression problem is to find a function $f({\\bf x})$ providing *good* predictions about some unknown variable $s$. To do so, we assume that a set of *labelled* training examples, $\\{{\\bf x}^{(k)}, s^{(k)}\\}_{k=1}^K$ is available. \n",
    "\n",
    "The predictor function should make good predictions for new observations ${\\bf x}$ not used during training. In practice, this is tested using a second set (the *test set*) of labelled samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NOTE: In the following, we will use capital letters, ${\\bf X}$, $S$, ..., to denote random variables, and lower-case letters ${\\bf x}$, s, ..., to the denote the values they can take. When there is no ambigüity, we will remove subindices of the density functions, $p_{{\\bf X}, S}({\\bf x}, s)= p({\\bf x}, s)$ to simplify the mathematical notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2. Model-based parametric regression\n",
    "\n",
    "Model-based regression methods assume that all data in the training and test dataset habe been generated by some stochastic process. In parametric regression, we assume that the probability distribution generating the data has a known parametric form, but the values of some parameters are unknown. \n",
    "\n",
    "In particular, in this notebook we will assume the target variables in all pairs $({\\bf x}^{(k)}, s^{(k)})$ from the training and test sets have been generated independently from some posterior distribution $p(s| {\\bf x}, {\\bf w})$, were ${\\bf w}$ is some unknown parameter. The training dataset is used to estimate ${\\bf w}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once $p(s|{\\bf x},{\\bf w})$ is known or can be estimated, Estimation Theory can be applied to estimate $s$ for any input ${\\bf x}$. For instance, any of these classical estimates can be used:\n",
    "\n",
    "* Maximum A Posterior (MAP): $\\qquad\\hat{s}_{\\text{MAP}} = \\arg\\max_s p(s| {\\bf x}, {\\bf w})$\n",
    "* Minimum Mean Square Error (MSE): $\\qquad\\hat{s}_{\\text{MSE}} = \\mathbb{E}\\{S |{\\bf x}, {\\bf w}\\}$\n",
    "\n",
    "<img src=\"figs/ParametricReg.png\", width=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1.3.1. Maximum Likelihood (ML) parameter estimation\n",
    "\n",
    "One way to estimate ${\\bf w}$ is to apply the maximum likelihood principle: take the value ${\\bf w}_\\text{ML}$ maximizing the joint distribution of the target variables given the inputs and given ${\\bf w}$, i.e.\n",
    "\n",
    "$$\n",
    "{\\bf w}_\\text{ML} = \\arg\\max_{\\bf w} p({\\bf s}|{\\bf X}, {\\bf w})\n",
    "$$\n",
    "\n",
    "where ${\\bf s} = \\left(s^{(1)}, \\dots, s^{(K)}\\right)^\\top$ is the vector of target variables and ${\\bf X} = \\left({\\bf x}^{(1)}, \\dots, {\\bf x}^{(K)}\\right)^\\top$ is the input matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NOTE: Since the training data inputs are known, all probability density functions and expectations in the remainder of this notebook will be conditioned on ${\\bf X}$. To simplify the mathematical notation, from now on we will remove ${\\bf X}$ from all conditions. Keep in mind that, in any case, all probabilities and expectations may depend on ${\\bf X}$ implicitely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1.3.2. The Gaussian case\n",
    "\n",
    "A particularly interesting case arises when the data model is Gaussian:\n",
    "\n",
    "$$p(s|{\\bf x}, {\\bf w}) = \n",
    "    \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\n",
    "    \\exp\\left(-\\frac{(s-{\\bf w}^\\top{\\bf z})^2}{2\\sigma_\\varepsilon^2}\\right)\n",
    "$$\n",
    "\n",
    "where ${\\bf z}=T({\\bf x})$ is a vector with components which can be computed directly from the observed variables. Such expression includes a linear regression model, where ${\\bf z} = [1; {\\bf x}]$, as well as any other non-linear model as long as it can be expressed as a <i>\"linear in the parameters\"</i> model.\n",
    "\n",
    "In that case, it can be shown that the likelihood function $p({\\bf s}| {\\bf w})$ ($\\equiv p({\\bf s}| {\\bf X}, {\\bf w})$) is given by\n",
    "\n",
    "$$\n",
    "p({\\bf s}| {\\bf w})\n",
    "    = \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}\\right)^K\n",
    "      \\exp\\left(-\\frac{1}{2\\sigma_\\varepsilon^2}\\|{\\bf s}-{\\bf Z}{\\bf w}\\|^2\\right)\n",
    "$$\n",
    "\n",
    "which is maximum for the Least Squares solution\n",
    "\n",
    "$$\n",
    "{\\bf w}_{ML} = ({\\bf Z}^\\top{\\bf Z})^{-1}{\\bf Z}^\\top{\\bf s}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.4. Limitations of the ML estimators.\n",
    "\n",
    "Since the ML estimation is equivalent to the LS solution under a Gaussian data model, it has the same drawbacks of LS regression. In particular, ML estimation is prone to overfiting. In general, if the number of parameters (i.e. the dimension of ${\\bf w}$) is large in relation to the size of the training data, the predictor based on the ML estimate may have a small square error over the training set but a large error over the test set. Therefore, in practice, som cross validation procedures is required to keep the complexity of the predictor function under control depending on the size of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Bayesian Regression\n",
    "\n",
    "One of the reasons why the ML estimate is prone to overfitting is that the prediction function uses ${\\bf w}_\\text{ML}$ without taking into account how much uncertain the true value of ${\\bf w}$ is.\n",
    "\n",
    "Bayesian methods utilize such information but considering ${\\bf w}$ as a random variable with some prior distribution $p({\\bf w})$. The posterior distribution $p({\\bf w}|{\\bf s})$ will be our measure of the uncertainty about the true value of the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In fact, this posterior distribution is a key component of the predictor function. Indeed, the minimum MSE estimate can be computed as\n",
    "\n",
    "$$\n",
    "\\hat{s}_\\text{MSE} \n",
    "   = \\mathbb{E}\\{s|{\\bf s}, {\\bf x}\\}\n",
    "   = \\int \\mathbb{E}\\{s|{\\bf w}, {\\bf s}, {\\bf x}\\} p({\\bf w}|{\\bf s}) d{\\bf w}\n",
    "$$\n",
    "\n",
    "Since the samples are i.i.d. $\\mathbb{E}\\{s|{\\bf w}, {\\bf s}, {\\bf x}\\} = \\mathbb{E}\\{s|{\\bf w}, {\\bf x}\\}$ and, thus\n",
    "\n",
    "$$\n",
    "\\hat{s}_\\text{MSE} \n",
    "   = \\int \\mathbb{E}\\{s|{\\bf w}, {\\bf x}\\} p({\\bf w}|{\\bf s}) d{\\bf w}\n",
    "$$\n",
    "\n",
    "Noting that $\\mathbb{E}\\{s|{\\bf w}, {\\bf s}, {\\bf x}\\}$ is the minimum MSE prediction for a given value of ${\\bf w}$, we observe that the Bayesian predictor is a weighted sum of these predictions, weighted by its posterior probability (density) of being the correct one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Posterior weight distribution\n",
    "\n",
    "We will express our <i>a priori</i> belief of models using a prior distribution $p({\\bf w})$. Then we can infer the <i>a posteriori</i> distribution using Bayes' rule:\n",
    "   \n",
    "   $$p({\\bf w}|{\\bf s}) = \\frac{p({\\bf s}|{\\bf w})~p({\\bf w})}{p({\\bf s})}$$\n",
    "   \n",
    "   Where:\n",
    "   - $p({\\bf s}|{\\bf w})$: is the likelihood function\n",
    "   - $p({\\bf w})$: is the <i>prior</i> distribution of the weights (assumptions are needed here)\n",
    "   - $p({\\bf s})$: is the <i>marginal</i> distribution of the observed data, which could be obtained integrating the expression in the numerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The previous expression can be interpreted in a rather intuitive way:\n",
    "\n",
    "   - Since ${\\bf w}$ are the parameters of the model, $p({\\bf w})$ express our belief about which models should be preferred over others before we see any data. For instance, since parameter vectors with small norms produce smoother curves, we could assign (<i>a priori</i>) a larger pdf value to models with smaller norms\n",
    "   \n",
    "   - The likelihood function $p({\\bf s}|{\\bf w})$ tells us how well the observations can be explained by a particular model\n",
    "   \n",
    "   - Finally, the posterior distribution $p({\\bf w}|{\\bf s})$ expresses the estimated goodness of each model (i.e., each parameter vector ${\\bf w}$) taking into consideration both the prior and the likelihood of $\\bf w$. Thus, a model with large $p({\\bf w})$ would have a low posterior value if it offers a poor explanation of the data (i.e., if $p({\\bf s}|{\\bf w})$ is small), whereas models that fit well with the observations would get emphasized\n",
    "   \n",
    "The posterior distribution of weights opens the door to working with several models at once. Rather thank keeping the estimated best model according to a certain criterion, we can now use all models parameterized by ${\\bf w}$, assigning them different degrees of confidence according to $p({\\bf w}|{\\bf s})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.1.1. A Gaussian Prior\n",
    "\n",
    "Since each value of ${\\bf w}$ determines a regression functions, by stating a prior distributions over the weights we state also a prior distribution over the space of regression functions.\n",
    "\n",
    "For instance, consider a particular example in which we assume a Gaussian prior for the weights given by:\n",
    "\n",
    "$${\\bf w} \\sim {\\cal N}\\left({\\bf 0},{\\pmb \\Sigma}_{p} \\right)$$\n",
    "\n",
    "Since the Gaussian distribution is larger around its mean, this prior distribution assigns more credibility to models with smaller $\\|{\\bf w}\\|$, i.e., to models that produce smoother curves $f({\\bf x}) = {\\bf w}^\\top {\\bf z}$.\n",
    "\n",
    "The following figure shows functions which are generated by drawing models from this distribution. You can check that faster oscillating regression curves can be obtained if the width of the Gaussian prior for $\\bf w$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_points = 20\n",
    "n_grid = 200\n",
    "frec = 3\n",
    "std_n = 0.2\n",
    "degree = 3\n",
    "nplots = 20\n",
    "\n",
    "#Prior distribution parameters\n",
    "sigma_eps = 0.1\n",
    "mean_w = np.zeros((degree+1,))\n",
    "sigma_w = 0.03  ### Try increasing this value\n",
    "var_w = sigma_w * np.eye(degree+1)\n",
    "\n",
    "X_tr = 3 * np.random.random((n_points,1)) - 0.5\n",
    "S_tr = - np.cos(frec*X_tr) + std_n * np.random.randn(n_points,1)\n",
    "\n",
    "xmin = np.min(X_tr)\n",
    "xmax = np.max(X_tr)\n",
    "X_grid = np.linspace(xmin-0.2*(xmax-xmin), xmax+0.2*(xmax-xmin),n_grid)\n",
    "S_grid = - np.cos(frec*X_grid) #Noise free for the true model\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(X_tr,S_tr,'b.',markersize=10)\n",
    "\n",
    "for k in range(nplots):\n",
    "    \n",
    "    #Draw weigths fromt the prior distribution\n",
    "    w_iter = np.random.multivariate_normal(mean_w,var_w)\n",
    "    S_grid_iter = np.polyval(w_iter,X_grid)\n",
    "    ax.plot(X_grid,S_grid_iter,'g-')\n",
    "\n",
    "ax.set_xlim(xmin-0.2*(xmax-xmin), xmax+0.2*(xmax-xmin))\n",
    "ax.set_ylim(S_tr[0]-2,S_tr[-1]+2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$s$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Summary\n",
    "\n",
    "Summarizing, the steps to design a Bayesian parametric regresion algorithm are the following:\n",
    "\n",
    "1. Assume a parametric data model $p(s| {\\bf x},{\\bf w})$ and a prior distribution $p({\\bf w})$.\n",
    "2. Using the data model and the i.i.d. assumption, compute $p({\\bf s}|{\\bf w})$.\n",
    "3. Applying the bayes rule, compute the posterior distribution $p({\\bf w}|{\\bf s})$.\n",
    "4. Compute the MSE estimate of $s$ given ${\\bf x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Bayesian regression for a Gaussian model.\n",
    "\n",
    "We will apply the above steps to derive a Bayesian regression algorithm for a Gaussian model.\n",
    "\n",
    "### 3.1. Step 1: The Gaussian model.\n",
    "\n",
    "Let as assume that the likelihood function is given by the Gaussian model described in Sec. 1.3.2.\n",
    "\n",
    "$$\n",
    "s~|~{\\bf w} \\sim {\\cal N}\\left({\\bf z}^\\top{\\bf w}, \\sigma_\\varepsilon^2 {\\bf I} \\right)\n",
    "$$\n",
    "\n",
    "and that the prior is also Gaussian\n",
    "\n",
    "$$\n",
    "{\\bf w} \\sim {\\cal N}\\left({\\bf 0},{\\pmb \\Sigma}_{p} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2. Step 2: Complete data likelihood\n",
    "\n",
    "Using the i.i.d. assumption, \n",
    "\n",
    "$$\n",
    "{\\bf s}~|~{\\bf w} \\sim {\\cal N}\\left({\\bf Z}{\\bf w},\\sigma_\\varepsilon^2 {\\bf I} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3. Step 3: Posterior weight distribution\n",
    "\n",
    "The posterior distribution of the weights can be computed using the Bayes rule\n",
    "\n",
    "$$p({\\bf w}|{\\bf s}) = \\frac{p({\\bf s}|{\\bf w})~p({\\bf w})}{p({\\bf s})}$$\n",
    "\n",
    "Since both $p({\\bf s}|{\\bf w})$ and $p({\\bf w})$ follow a Gaussian distribution, we know also that the joint distribution and the posterior distribution of ${\\bf w}$ given ${\\bf s}$ are also Gaussian. Therefore,\n",
    "\n",
    "$${\\bf w}~|~{\\bf s} \\sim {\\cal N}\\left({\\bf w}_\\text{MSE}, {\\pmb\\Sigma}_{\\bf w}\\right)$$\n",
    "\n",
    "After some algebra, it can be shown that mean and the covariance matrix of the distribution are:\n",
    "\n",
    "$${\\pmb\\Sigma}_{\\bf w} = \\left[\\frac{1}{\\sigma_\\varepsilon^2} {\\bf Z}^{\\top}{\\bf Z} + {\\pmb \\Sigma}_p^{-1}\\right]^{-1}$$\n",
    "\n",
    "$${\\bf w}_\\text{MSE} = {\\sigma_\\varepsilon^{-2}} {\\pmb\\Sigma}_{\\bf w} {\\bf Z}^\\top {\\bf s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1: \n",
    "\n",
    "Consider the dataset with one-dimensional inputs given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# True data parameters\n",
    "w_true = 3\n",
    "std_n = 0.4\n",
    "\n",
    "# Generate the whole dataset\n",
    "n_max = 64\n",
    "X_tr = 3 * np.random.random((n_max,1)) - 0.5\n",
    "S_tr =  w_true * X_tr + std_n * np.random.randn(n_max,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fit a Bayesian linear regression model assuming ${\\bf z}={\\bf x}$ and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "sigma_eps = 0.1\n",
    "mean_w = np.zeros((1,))\n",
    "sigma_p = 1e6 * np.eye(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To do so, compute the posterior weight distribution using the first $k$ samples in the complete dataset, for $k = 1,2,4,8,\\ldots, 128$. Draw all these posteriors along with the prior distribution in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# No. of points to analyze\n",
    "n_points = [1, 2, 4, 8, 16, 32, 64]\n",
    "\n",
    "# Prepare plots\n",
    "w_grid = np.linspace(2.7, 3.4, 5000)   # Sample the w axiss\n",
    "plt.figure()\n",
    "\n",
    "# Plot the prior distribution\n",
    "# p = <FILL IN>\n",
    "\n",
    "plt.plot(w_grid, p.flatten(),'g-')\n",
    "\n",
    "for k in n_points:\n",
    "\n",
    "    # Select the first k samples\n",
    "    Zk = X_tr[0:k, :]\n",
    "    Sk = S_tr[0:k]\n",
    "\n",
    "    # Compute the parameters of the posterior distribution\n",
    "    # Sigma_w = <FILL IN>\n",
    "    # w_MSE = <FILL IN>\n",
    "\n",
    "    w_MSE = np.array(w_MSE).flatten()\n",
    "\n",
    "    # Draw weights from the posterior distribution\n",
    "    # p = <FILL IN>\n",
    "\n",
    "    p = p.flatten()\n",
    "    \n",
    "    plt.plot(w_grid, p,'g-')\n",
    "    plt.fill_between(w_grid, 0, p, alpha=0.8, edgecolor='#1B2ACC', facecolor='#089FFF',\n",
    "        linewidth=1, antialiased=True)\n",
    "    plt.xlim(w_grid[0], w_grid[-1])\n",
    "    plt.ylim(0, np.max(p))\n",
    "    plt.xlabel('$w$')\n",
    "    plt.ylabel('$p(w|s)$')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Remove the temporary plots and fix the last one\n",
    "display.clear_output(wait=True)\n",
    "plt.show()\n",
    "\n",
    "# Print the weight estimate based on the whole dataset\n",
    "print w_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2: \n",
    "\n",
    "Note that, in the example above, the model assumptions are correct: the target variables have been generated by a linear model with noise standard deviation `sigma_n` which is exactly equal to the value assumed by the model, stored in variable `sigma_eps`. Check what happens if we take `sigma_eps=4*sigma_n` or `sigma_eps=sigma_n/4`. \n",
    "\n",
    "* Does the algorithm fails in that cases?\n",
    "* What differences can you observe with respect to the ideal case `sigma_eps=sigma_n`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4. Step 4: MSE estimate\n",
    "\n",
    "Noting that \n",
    "$$\n",
    "\\mathbb{E}\\{s|{\\bf w}, {\\bf x}\\} = {\\bf w}^\\top {\\bf z}\n",
    "$$\n",
    "\n",
    "we can write\n",
    "\n",
    "$$\n",
    "\\hat{s}_\\text{MSE} \n",
    "   = \\int {\\bf w}^\\top {\\bf z} p({\\bf w}|{\\bf s}) d{\\bf w}\n",
    "   = \\left(\\int {\\bf w} p({\\bf w}|{\\bf s}) d{\\bf w}\\right)^\\top {\\bf z}\n",
    "   = {\\bf w}_\\text{MSE}^\\top {\\bf z}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "{\\bf w}_\\text{MSE} \n",
    "   = \\int {\\bf w} p({\\bf w}|{\\bf s}) d{\\bf w}\n",
    "   = {\\sigma_\\varepsilon^{-2}} {\\pmb\\Sigma}_{\\bf w} {\\bf Z}^\\top {\\bf s}\n",
    "$$\n",
    "\n",
    "Therefore, in the Gaussian case, the weighted integration of prediction function is equivalent to apply a unique model, with weights ${\\bf w}_\\text{MSE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Exercise 3:\n",
    "\n",
    "Plot the minimum MSE predictions of $s$ for inputs $x$ in the interval [-1, 3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.5 Maximum likelihood vs Bayesian Inference.  Making predictions\n",
    "\n",
    "   - Following an <b>ML approach</b>, we retain a single model, ${\\bf w}_{ML} = \\arg \\max_{\\bf w} p({\\bf s}|{\\bf w})$. Then, the predictive distribution of the target value for a new point would be obtained as:\n",
    "   \n",
    "   $$p({s^*}|{\\bf w}_{ML},{\\bf x}^*) $$\n",
    "   \n",
    "   For the generative model of Section 3.1.2 (additive i.i.d. Gaussian noise), this distribution is:\n",
    "   \n",
    "   $$p({s^*}|{\\bf w}_{ML},{\\bf x}^*) = \\frac{1}{\\sqrt{2\\pi\\sigma_\\varepsilon^2}} \\exp \\left(-\\frac{\\left(s^* - {\\bf w}_{ML}^\\top {\\bf z}^*\\right)^2}{2 \\sigma_\\varepsilon^2} \\right)$$\n",
    "   \n",
    "      * The mean of $s^*$ is just the same as the prediction of the LS model, and the same uncertainty is assumed independently of the observation vector (i.e., the variance of the noise of the model).\n",
    "      \n",
    "      * If a single value is to be kept, we would probably keep the mean of the distribution, which is equivalent to the LS prediction.\n",
    "   \n",
    "   \n",
    "   - Using <b>Bayesian inference</b>, we retain all models. Then, the inference of the value $s^* = s({\\bf x}^*)$ is carried out by mixing all models, according to the weights given by the posterior distribution.\n",
    "   \n",
    "   \\begin{align}p({s^*}|{\\bf x}^*,{\\bf s}) \n",
    "                         & = \\int p({s^*}~|~{\\bf w},{\\bf x}^*) p({\\bf w}~|~{\\bf s}) d{\\bf w}\\end{align}\n",
    "   \n",
    "   where:\n",
    "   \n",
    "      * $p({s^*}|{\\bf w},{\\bf x}^*) = \\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma_\\varepsilon^2}} \\exp \\left(-\\frac{\\left(s^* - {\\bf w}^\\top {\\bf z}^*\\right)^2}{2 \\sigma_\\varepsilon^2} \\right)$\n",
    "      * $p({\\bf w}~|~{\\bf s})$: Is the posterior distribution of the weights, that can be computed using Bayes' Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following fragment of code draws random vectors from $p({\\bf w}|{\\bf s})$, and plots the corresponding regression curves along with the training points. Compare these curves with those extracted from the prior distribution of ${\\bf w}$ and with the LS solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_points = 15\n",
    "n_grid = 200\n",
    "frec = 3\n",
    "std_n = 0.2\n",
    "degree = 12\n",
    "nplots = 6\n",
    "\n",
    "#Prior distribution parameters\n",
    "sigma_eps = 0.1\n",
    "mean_w = np.zeros((degree+1,))\n",
    "sigma_p = .3 * np.eye(degree+1)\n",
    "\n",
    "X_tr = 3 * np.random.random((n_points,1)) - 0.5\n",
    "S_tr = - np.cos(frec*X_tr) + std_n * np.random.randn(n_points,1)\n",
    "X_grid = np.linspace(-.5,2.5,n_grid)\n",
    "S_grid = - np.cos(frec*X_grid) #Noise free for the true model\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(X_tr,S_tr,'b.',markersize=10)\n",
    "\n",
    "# Compute matrix with training input data for the polynomial model\n",
    "Z = []\n",
    "for x_val in X_tr.tolist():\n",
    "    Z.append([x_val[0]**k for k in range(degree+1)])\n",
    "Z=np.asmatrix(Z)\n",
    "\n",
    "#Compute posterior distribution parameters\n",
    "Sigma_w = np.linalg.inv(np.dot(Z.T,Z)/(sigma_eps**2) + np.linalg.inv(sigma_p))\n",
    "posterior_mean = Sigma_w.dot(Z.T).dot(S_tr)/(sigma_eps**2)\n",
    "posterior_mean = np.array(posterior_mean).flatten()\n",
    "\n",
    "for k in range(nplots):\n",
    "    \n",
    "    #Draw weights from the posterior distribution\n",
    "    w_iter = np.random.multivariate_normal(posterior_mean,Sigma_w)\n",
    "    #Note that polyval assumes the first element of weight vector is the coefficient of\n",
    "    #the highest degree term. Thus, we need to reverse w_iter\n",
    "    S_grid_iter = np.polyval(w_iter[::-1],X_grid)\n",
    "    ax.plot(X_grid,S_grid_iter,'g-')\n",
    "\n",
    "#We plot also the least square solution\n",
    "w_LS = np.polyfit(X_tr.flatten(), S_tr.flatten(), degree)\n",
    "S_grid_iter = np.polyval(w_LS,X_grid)\n",
    "ax.plot(X_grid,S_grid_iter,'m-',label='LS regression')\n",
    "    \n",
    "ax.set_xlim(-.5,2.5)\n",
    "ax.set_ylim(S_tr[0]-2,S_tr[-1]+2)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Posterior distribution of the target \n",
    "\n",
    "   - Since $f^* = f({\\bf x}^*) = {\\bf w}^\\top{\\bf z}$, $f^*$ is also a Gaussian variable whose posterior mean and variance can be calculated as follows:\n",
    "   \n",
    "   $$\\mathbb{E}\\{{{\\bf z}^*}^\\top {\\bf w}~|~{\\bf s}, {\\bf z}^*\\} = {{\\bf z}^*}^\\top \\mathbb{E}\\{{\\bf w}|{\\bf s}\\} = {\\sigma_\\varepsilon^{-2}} {{\\bf z}^*}^\\top  {\\pmb\\Sigma}_{\\bf w} {\\bf Z}^\\top {\\bf s}$$\n",
    "   \n",
    "   $$\\text{Cov}\\left[{{\\bf z}^*}^\\top {\\bf w}~|~{\\bf s}, {\\bf z}^*\\right] = {{\\bf z}^*}^\\top \\text{Cov}\\left[{\\bf w}~|~{\\bf s}\\right] {{\\bf z}^*} = {{\\bf z}^*}^\\top {\\pmb \\Sigma}_{\\bf w} {{\\bf z}^*}$$\n",
    "   \n",
    "   \n",
    "   - Therefore, $f^*~|~{\\bf s}, {\\bf x}^* \\sim {\\cal N}\\left({\\sigma_\\varepsilon^{-2}} {{\\bf z}^*}^\\top  {\\pmb\\Sigma}_{\\bf w} {\\bf Z}^\\top {\\bf s}, {{\\bf z}^*}^\\top {\\pmb \\Sigma}_{\\bf w} {{\\bf z}^*} \\right)$\n",
    "   \n",
    "   - Finally, for $s^* = f^* + \\varepsilon^*$, the posterior distribution is  $s^*~|~{\\bf s}, {\\bf z}^* \\sim {\\cal N}\\left({\\sigma_\\varepsilon^{-2}} {{\\bf z}^*}^\\top  {\\pmb\\Sigma}_{\\bf w} {\\bf Z}^\\top {\\bf s}, {{\\bf z}^*}^\\top {\\pmb \\Sigma}_{\\bf w} {{\\bf z}^*} + \\sigma_\\varepsilon^2\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_points = 15\n",
    "n_grid = 200\n",
    "frec = 3\n",
    "std_n = 0.2\n",
    "degree = 12\n",
    "nplots = 6\n",
    "\n",
    "#Prior distribution parameters\n",
    "sigma_eps = 0.1\n",
    "mean_w = np.zeros((degree+1,))\n",
    "sigma_p = .5 * np.eye(degree+1)\n",
    "\n",
    "X_tr = 3 * np.random.random((n_points,1)) - 0.5\n",
    "S_tr = - np.cos(frec*X_tr) + std_n * np.random.randn(n_points,1)\n",
    "X_grid = np.linspace(-1,3,n_grid)\n",
    "S_grid = - np.cos(frec*X_grid) #Noise free for the true model\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(X_tr,S_tr,'b.',markersize=10)\n",
    "\n",
    "#Compute matrix with training input data for the polynomial model\n",
    "Z = []\n",
    "for x_val in X_tr.tolist():\n",
    "    Z.append([x_val[0]**k for k in range(degree+1)])\n",
    "Z=np.asmatrix(Z)\n",
    "\n",
    "#Compute posterior distribution parameters\n",
    "Sigma_w = np.linalg.inv(np.dot(Z.T,Z)/(sigma_eps**2) + np.linalg.inv(sigma_p))\n",
    "posterior_mean = Sigma_w.dot(Z.T).dot(S_tr)/(sigma_eps**2)\n",
    "posterior_mean = np.array(posterior_mean).flatten()\n",
    "\n",
    "#Plot the posterior mean\n",
    "#Note that polyval assumes the first element of weight vector is the coefficient of\n",
    "#the highest degree term. Thus, we need to reverse w_iter\n",
    "S_grid_iter = np.polyval(posterior_mean[::-1],X_grid)\n",
    "ax.plot(X_grid,S_grid_iter,'g-',label='Predictive mean, BI')\n",
    "\n",
    "#Plot confidence intervals for the Bayesian Inference\n",
    "std_x = []\n",
    "for el in X_grid:\n",
    "    x_ast = np.array([el**k for k in range(degree+1)])\n",
    "    std_x.append(np.sqrt(x_ast.dot(Sigma_w).dot(x_ast)[0,0]))\n",
    "std_x = np.array(std_x)\n",
    "plt.fill_between(X_grid, S_grid_iter-std_x, S_grid_iter+std_x,\n",
    "    alpha=0.2, edgecolor='#1B2ACC', facecolor='#089FFF',\n",
    "    linewidth=4, linestyle='dashdot', antialiased=True)\n",
    "\n",
    "#We plot also the least square solution\n",
    "w_LS = np.polyfit(X_tr.flatten(), S_tr.flatten(), degree)\n",
    "S_grid_iter = np.polyval(w_LS,X_grid)\n",
    "ax.plot(X_grid,S_grid_iter,'m-',label='LS regression')\n",
    "    \n",
    "ax.set_xlim(-1,3)\n",
    "ax.set_ylim(S_tr[0]-2,S_tr[-1]+2)\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not only do we obtain a better predictive model, but we also have confidence intervals (error bars) for the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4 Maximum evidence model selection\n",
    "\n",
    "We have already addressed with Bayesian Inference the following two issues:\n",
    "\n",
    "   - For a given degree, how do we choose the weights?\n",
    "   \n",
    "   - Should we focus on just one model, or can we use several models at once?\n",
    "   \n",
    "However, we still needed some assumptions: a parametric model (i.e., polynomial function and <i>a priori</i> degree selection) and several parameters needed to be adjusted.\n",
    "\n",
    "Though we can recur to cross-validation, Bayesian inference opens the door to other strategies. \n",
    "\n",
    "   - We could argue that rather than keeping single selections of these parameters, we could use simultaneously several sets of parameters (and/or several parametric forms), and average them in a probabilistic way ... (like we did with the models)\n",
    "   \n",
    "   - We will follow a simpler strategy, selecting just the most likely set of parameters according to an ML criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1 Model evidence\n",
    "\n",
    "The evidence of a model is defined as\n",
    "\n",
    "$$L = p({\\bf s}~|~{\\cal M})$$\n",
    "\n",
    "where ${\\cal M}$ denotes the model itself and any free parameters it may have. For instance, for the polynomial model we have assumed so far, ${\\cal M}$ would represent the degree of the polynomia, the variance of the additive noise, and the <i>a priori</i> covariance matrix of the weights\n",
    "\n",
    "Applying the Theorem of Total probability, we can compute the evidence of the model as\n",
    "\n",
    "$$L = \\int p({\\bf s}~|~{\\bf f},{\\cal M}) p({\\bf f}~|~{\\cal M}) d{\\bf f} $$\n",
    "\n",
    "For the linear model $f({\\bf x}) = {\\bf w}^\\top{\\bf z}$, the evidence can be computed as\n",
    "\n",
    "$$L = \\int p({\\bf s}~|~{\\bf w},{\\cal M}) p({\\bf w}~|~{\\cal M}) d{\\bf w} $$\n",
    "\n",
    "It is important to notice that these probability density functions are exactly the ones we computed on the previous section. We are just making explicit that they depend on a particular model and the selection of its parameters. Therefore:\n",
    "\n",
    "   - $p({\\bf s}~|~{\\bf w},{\\cal M})$ is the likelihood of ${\\bf w}$\n",
    "   \n",
    "   - $p({\\bf w}~|~{\\cal M})$ is the <i>a priori</i> distribution of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2 Model selection via evidence maximization\n",
    "\n",
    "   - As we have already mentioned, we could propose a prior distribution for the model parameters, $p({\\cal M})$, and use it to infer the posterior. However, this can be very involved (usually no closed-form expressions can be derived)\n",
    "   \n",
    "   - Alternatively, maximizing the evidence is normally good enough\n",
    "   \n",
    "   $${\\cal M}_{ML} = \\arg\\max_{\\cal M} p(s~|~{\\cal M})$$\n",
    "   \n",
    "Note that we are using the subscript 'ML' because the evidence can also be referred to as the likelihood of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.3 Example: Selection of the degree of the polynomia\n",
    "\n",
    "For the previous example we had (we consider a spherical Gaussian for the weights):\n",
    "\n",
    "   - ${\\bf s}~|~{\\bf w},{\\cal M}~\\sim~{\\cal N}\\left({\\bf Z}{\\bf w},\\sigma_\\varepsilon^2 {\\bf I} \\right)$\n",
    "   \n",
    "   - ${\\bf w}~|~{\\cal M}~\\sim~{\\cal N}\\left({\\bf 0},\\sigma_p^2 {\\bf I} \\right)$\n",
    "   \n",
    "In this case, $p({\\bf s}~|~{\\cal M})$ follows also a Gaussian distribution, and it can be shown that\n",
    "\n",
    "   - $L = p({\\bf s}~|~{\\cal M}) = {\\cal N}\\left({\\bf 0},\\sigma_p^2 {\\bf Z} {\\bf Z}^\\top+\\sigma_\\varepsilon^2 {\\bf I} \\right)$\n",
    "   \n",
    "If we just pursue the maximization of $L$, this is equivalent to maximizing the log of the evidence\n",
    "\n",
    "$$\\log(L) = -\\frac{M}{2} \\log(2\\pi) -{\\frac{1}{2}}\\log\\mid\\sigma_p^2 {\\bf Z} {\\bf Z}^\\top+\\sigma_\\varepsilon^2 {\\bf I}\\mid - \\frac{1}{2} {\\bf s}^\\top \\left(\\sigma_p^2 {\\bf Z} {\\bf Z}^\\top+\\sigma_\\varepsilon^2 {\\bf I}\\right)^{-1} {\\bf s}$$\n",
    "\n",
    "where $M$ denotes the length of vector ${\\bf z}$ (the degree of the polynomia minus 1).\n",
    "   \n",
    "The following fragment of code evaluates the evidence of the model as a function of the degree of the polynomia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "n_points = 15\n",
    "frec = 3\n",
    "std_n = 0.2\n",
    "max_degree = 12\n",
    "\n",
    "#Prior distribution parameters\n",
    "sigma_eps = 0.2\n",
    "mean_w = np.zeros((degree+1,))\n",
    "sigma_p = 0.5\n",
    "\n",
    "X_tr = 3 * np.random.random((n_points,1)) - 0.5\n",
    "S_tr = - np.cos(frec*X_tr) + std_n * np.random.randn(n_points,1)\n",
    "\n",
    "#Compute matrix with training input data for the polynomial model\n",
    "Z = []\n",
    "for x_val in X_tr.tolist():\n",
    "    Z.append([x_val[0]**k for k in range(degree+1)])\n",
    "Z=np.asmatrix(Z)\n",
    "\n",
    "#Evaluate the posterior evidence\n",
    "\n",
    "logE = []\n",
    "for deg in range(max_degree):\n",
    "    Z_iter = Z[:,:deg+1]\n",
    "    logE_iter = -((deg+1)*np.log(2*pi)/2) \\\n",
    "                -np.log(np.linalg.det((sigma_p**2)*Z_iter.dot(Z_iter.T) + (sigma_eps**2)*np.eye(n_points)))/2 \\\n",
    "                -S_tr.T.dot(np.linalg.inv((sigma_p**2)*Z_iter.dot(Z_iter.T) + (sigma_eps**2)*np.eye(n_points))).dot(S_tr)/2\n",
    "    logE.append(logE_iter[0,0])\n",
    "\n",
    "plt.plot(np.array(range(max_degree))+1,logE)\n",
    "plt.xlabel('Polynomia degree')\n",
    "plt.ylabel('log evidence')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The above curve may change the position of its maximum from run to run.\n",
    "\n",
    "We conclude the notebook by plotting the result of the Bayesian inference for M=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_points = 15\n",
    "n_grid = 200\n",
    "frec = 3\n",
    "std_n = 0.2\n",
    "degree = 5 #M-1\n",
    "nplots = 6\n",
    "\n",
    "#Prior distribution parameters\n",
    "sigma_eps = 0.1\n",
    "mean_w = np.zeros((degree+1,))\n",
    "sigma_p = .5 * np.eye(degree+1)\n",
    "\n",
    "X_tr = 3 * np.random.random((n_points,1)) - 0.5\n",
    "S_tr = - np.cos(frec*X_tr) + std_n * np.random.randn(n_points,1)\n",
    "X_grid = np.linspace(-1,3,n_grid)\n",
    "S_grid = - np.cos(frec*X_grid) #Noise free for the true model\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(X_tr,S_tr,'b.',markersize=10)\n",
    "\n",
    "#Compute matrix with training input data for the polynomial model\n",
    "Z = []\n",
    "for x_val in X_tr.tolist():\n",
    "    Z.append([x_val[0]**k for k in range(degree+1)])\n",
    "Z=np.asmatrix(Z)\n",
    "\n",
    "#Compute posterior distribution parameters\n",
    "Sigma_w = np.linalg.inv(np.dot(Z.T,Z)/(sigma_eps**2) + np.linalg.inv(sigma_p))\n",
    "posterior_mean = Sigma_w.dot(Z.T).dot(S_tr)/(sigma_eps**2)\n",
    "posterior_mean = np.array(posterior_mean).flatten()\n",
    "\n",
    "#Plot the posterior mean\n",
    "#Note that polyval assumes the first element of weight vector is the coefficient of\n",
    "#the highest degree term. Thus, we need to reverse w_iter\n",
    "S_grid_iter = np.polyval(posterior_mean[::-1],X_grid)\n",
    "ax.plot(X_grid,S_grid_iter,'g-',label='Predictive mean, BI')\n",
    "\n",
    "#Plot confidence intervals for the Bayesian Inference\n",
    "std_x = []\n",
    "for el in X_grid:\n",
    "    x_ast = np.array([el**k for k in range(degree+1)])\n",
    "    std_x.append(np.sqrt(x_ast.dot(Sigma_w).dot(x_ast)[0,0]))\n",
    "std_x = np.array(std_x)\n",
    "plt.fill_between(X_grid, S_grid_iter-std_x, S_grid_iter+std_x,\n",
    "    alpha=0.2, edgecolor='#1B2ACC', facecolor='#089FFF',\n",
    "    linewidth=4, linestyle='dashdot', antialiased=True)\n",
    "\n",
    "#We plot also the least square solution\n",
    "w_LS = np.polyfit(X_tr.flatten(), S_tr.flatten(), degree)\n",
    "S_grid_iter = np.polyval(w_LS,X_grid)\n",
    "ax.plot(X_grid,S_grid_iter,'m-',label='LS regression')\n",
    "    \n",
    "ax.set_xlim(-1,3)\n",
    "ax.set_ylim(S_tr[0]-2,S_tr[-1]+2)\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can check, that now the model also seems quite appropriate for LS regression, but keep in mind that selection of such parameter was itself carried out using Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
